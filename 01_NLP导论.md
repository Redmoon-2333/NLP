# 第1章 NLP导论

## 写在前面：NLP入门必知

在正式开始学习NLP之前，让我们先了解一些基本概念。

### ⚠️ 初学者常见困惑

**Q1：学习NLP需要什么基础？**
```
必须：
- Python编程基础
- 基本的数学知识（线性代数、概率论）

推荐：
- 机器学习基础（但不是必须）
- 深度学习框架（PyTorch/TensorFlow）

不需要：
- 语言学背景（会更好，但不必须）
```

**Q2：NLP和机器学习、深度学习是什么关系？**
```
关系图：

  人工智能 (AI)
      │
      ├── 机器学习 (ML)
      │       │
      │       ├── 深度学习 (DL)
      │       │       │
      │       │       └── 神经网络
      │       │
      │       └── 传统 ML（SVM、决策树等）
      │
      ├── 计算机视觉 (CV)
      ├── 自然语言处理 (NLP)  ← 我们在这里
      └── 语音识别

NLP是AI的一个分支，会用到机器学习和深度学习的技术
```

**Q3：NLP的学习路径是什么？**
```
建议的学习顺序：

第1阶段：基础概念
├─ 什么是NLP
├─ 分词、词性标注
├─ 文本表示（One-Hot、词向量）
└─ 常见任务

第2阶段：经典模型
├─ Word2Vec / GloVe
├─ RNN / LSTM
├─ 注意力机制
└─ Seq2Seq

第3阶段：现代大模型
├─ Transformer
├─ BERT / GPT
├─ 预训练+微调
└─ Prompt Engineering

第4阶段：实战应用
├─ 具体任务实践
├─ 模型部署
└─ 项目开发
```

### 📖 本文档的阅读指南

**符号说明：**
- ✅ 优点/好处
- ❌ 缺点/问题
- 💡 重点内容
- ⚠️ 注意事项

**代码块说明：**
```python
# Python代码示例
```

```
# 伪代码或文本示例
```

**学习建议：**
1. 逐章阅读，不要跳过基础内容
2. 动手实践代码示例
3. 理解概念后再往下学
4. 遇到问题可回头复习

---

## 1.1 定义

### 1.1.1 什么是NLP

**自然语言处理（Natural Language Processing，NLP）** 是人工智能和语言学领域的交叉学科，旨在让计算机能够理解、解释和生成人类语言。

**核心目标：**
- 使计算机能够"读懂"人类语言（理解）
- 使计算机能够"说出"人类语言（生成）
- 实现人机之间的自然交互

**通俗理解：**

NLP就是教计算机"学说人话"。想想我们每天使用的：
- **智能语音助手**：Siri、小爱同学能听懂你的问题并回答
- **翻译软件**：谷歌翻译、有道翻译能把英文变中文
- **输入法**：拼音输入时的智能联想
- **搜索引擎**：百度、谷歌能理解你的搜索意图

这些NLP应用都已经融入我们的日常生活。

**NLP的挑战性：**

人类语言非常复杂，与编程语言不同：

1. **歧义性**
   ```
   "我在银行工作" → bank = 银行
   "I'm sitting on the river bank" → bank = 河岸
   ```

2. **上下文依赖**
   ```
   "他喜欢吃苹果" → 苹果是水果
   "苹果公司发布新产品" → 苹果是公司
   ```

3. **多样性**
   ```
   同一个意思有无数说法：
   "我想吃饭" = "我饿了" = "我需要进食" = "能给我点吃的吗"
   ```

4. **隐含信息**
   ```
   "房间有点凉"
   表面：陈述温度
   实际：可能是请求关窗/开空调
   ```

**NLP的核心任务：**
- 把人类的语言文字转换为计算机能处理的形式（数字、向量）
- 让计算机理解这些语言的含义
- 让计算机生成符合人类习惯的语言

### 1.1.2 NLP的研究范畴

NLP涵盖多个层次的语言处理任务：

| 层次 | 研究内容 | 典型任务 |
|------|---------|---------|
| **词法分析** | 词的结构和形态 | 分词、词性标注、命名实体识别 |
| **句法分析** | 句子的语法结构 | 句法树构建、依存句法分析 |
| **语义分析** | 词句的含义理解 | 词义消歧、语义角色标注 |
| **篇章分析** | 段落和文档层面 | 指代消解、篇章结构分析 |
| **应用层** | 实际任务 | 机器翻译、问答系统、情感分析 |

### 1.1.3 NLP vs 相关领域

**NLP与计算语言学（Computational Linguistics）：**
- 计算语言学：更注重语言学理论的计算建模
- NLP：更注重工程应用和实际效果

**NLP与文本挖掘（Text Mining）：**
- 文本挖掘：从大规模文本中发现知识和模式
- NLP：提供文本挖掘的底层技术支持

**NLP与语音识别（Speech Recognition）：**
- 语音识别：将声音转换为文字（语音→文本）
- NLP：处理文本层面的理解和生成

---

## 1.2 常见任务

### 1.2.1 基础任务

#### （1）分词（Word Segmentation）
**定义：** 将连续的文本序列切分成有意义的词汇单元。

**通俗解释：**
分词就像把一串连在一起的文字“切开”，变成一个个独立的词。

**中英文差异：**
- 英文：天然以空格分隔，相对简单
  ```
  "I love NLP" → ["I", "love", "NLP"]  # 直接按空格切分
  ```

- 中文：没有天然分隔符，需要算法识别词边界
  ```
  "我爱自然语言处理" → ?
  
  可能结果1：["我", "爱", "自然语言", "处理"]
  可能结果2：["我", "爱", "自然", "语言", "处理"]
  ```

**为什么分词很重要？**

分词是NLP的第一步，就像做菜前先要处理食材一样。如果分词错误，后面所有处理都会出问题。

```
错误分词的影响：
原文："南京市长江大桥"

正确分词：["南京市", "长江大桥"]
意思：南京市的长江大桥

错误分词：["南京", "市长", "江大桥"]
意思被歪曲：南京的市长姓江？
```

**示例：**
```
输入："我爱自然语言处理"
输出：["我", "爱", "自然语言", "处理"]
或：  ["我", "爱", "自然", "语言", "处理"]
```

**常用工具：** jieba、pkuseg、LTP

**Python快速上手：**
```python
import jieba

text = "我爱自然语言处理"
words = jieba.cut(text)
print('/'.join(words))  # 输出：我/爱/自然语言/处理
```

#### （2）词性标注（Part-of-Speech Tagging）
**定义：** 为每个词标注其语法类别（名词、动词、形容词等）。

**通俗解释：**
词性标注就像给每个词贴上一个“身份标签”，告诉计算机这个词是什么类型。

**为什么需要词性标注？**

1. **消除歧义**
   ```
   "吃的苹果" → "苹果"是名词（水果）
   "苹果色的衬衫" → "苹果"是形容词（颜色）
   ```

2. **理解句子结构**
   ```
   "我/代词 爱/动词 你/代词"
   主语(我) + 谓语(爱) + 宾语(你)
   ```

3. **提高后续任务准确率**
   - 命名实体识别：通常从名词中寻找
   - 情感分析：形容词和副词常包含情感信息

**示例：**
```
输入："我/爱/自然语言/处理"
输出："我/rr 爱/v 自然语言/n 处理/vn"

详细解释：
我        → rr  (代词 - pronoun)
爱        → v   (动词 - verb)
自然语言  → n   (名词 - noun)
处理      → vn  (名动词 - verbal noun，既可以作动词也可以作名词）
```

**词性标签示例：**
- n：名词（noun） - “苹果”、“桌子”
- v：动词（verb） - “吃”、“跑”
- a：形容词（adjective） - “美丽”、“高大”
- r：代词（pronoun） - “我”、“你”
- d：副词（adverb） - “很”、“非常”
- p：介词（preposition） - “在”、“从”

**Python快速上手：**
```python
import jieba.posseg as pseg

text = "我爱自然语言处理"
words = pseg.cut(text)

for word, flag in words:
    print(f"{word}/{flag}", end=" ")
# 输出：我/r 爱/v 自然语言/l 处理/v
```

#### （3）命名实体识别（Named Entity Recognition, NER）
**定义：** 识别文本中具有特定意义的实体，如人名、地名、机构名等。

**通俗解释：**
NER就像从文字中“找重点”，把重要的人名、地名、公司名等标记出来。

**生活中的应用：**

1. **智能助手**
   ```
   用户：“帮我查一下北京的天气”
   系统识别：[北京] ← 地点
   系统行动：查询北京天气
   ```

2. **新闻推荐**
   ```
   文章：“习近平在北京人民大会堂发表重要讲话”
   识别：[习近平-人名] [北京-地点] [人民大会堂-地点]
   用途：自动分类、相关新闻推荐
   ```

3. **信息抽取**
   ```
   合同：“甲方阿里巴巴集团与乙方腾讯科技签署协议...”
   识别：[阿里巴巴集团-组织] [腾讯科技-组织]
   构建：知识图谱、实体关系
   ```

**示例：**
```
输入："苹果公司的CEO蒂姆·库克在加州宣布新产品"

输出（标注结果）：
  - 苹果公司 [组织/ORG]
  - 蒂姆·库克 [人名/PER]
  - 加州 [地点/LOC]

视觉化显示：
“<ORG>苹果公司</ORG>的CEO<PER>蒂姆·库克</PER>在<LOC>加州</LOC>宣布新产品”
```

**常见实体类型：**
- PER（Person）：人名 - “马云”、“李白”
- LOC（Location）：地点 - “北京”、“长城”
- ORG（Organization）：组织机构 - “阿里巴巴”、“联合国”
- TIME：时间表达 - “2024年2月6日”、“明天”
- DATE：日期 - “今年”、“上个月”
- MONEY：货币金额 - “100元”、“5美元”

**NER的难点：**

1. **实体边界模糊**
   ```
   "北京大学" vs "北京/大学"
   是一个机构名？还是地点+通称？
   ```

2. **同名不同类**
   ```
   "苹果" → 可能是公司名（Apple）
   "苹果" → 也可能是水果（不是实体）
   ```

3. **嵌套实体**
   ```
   "北京市海淀区人民医院"
   [北京市-LOC] [海淀区-LOC] 嵌套在 [北京市海淀区人民医院-ORG]
   ```

**Python快速上手：**
```python
import spacy

# 加载中文模型
nlp = spacy.load("zh_core_web_sm")

text = "苹果公司的CEO蒂姆·库克在加州宣布新产品"
doc = nlp(text)

# 提取命名实体
for ent in doc.ents:
    print(f"{ent.text} [{ent.label_}]")

# 输出：
# 苹果公司 [ORG]
# 蒂姆·库克 [PERSON]
# 加州 [GPE]
```

### 1.2.2 理解类任务

#### （1）情感分析（Sentiment Analysis）
**定义：** 判断文本表达的情感倾向（正面、负面、中性）。

**应用场景：**
- 电商评论分析
- 社交媒体舆情监测
- 客户反馈分析

**示例：**
```
"这部电影太精彩了！" → 正面（积极）
"服务态度很差，不推荐。" → 负面（消极）
```

**细粒度情感分析：**
- 二分类：正面/负面
- 三分类：正面/中性/负面
- 多分类：非常满意/满意/一般/不满意/非常不满意

#### （2）文本分类（Text Classification）
**定义：** 将文本分配到预定义的类别中。

**典型应用：**
- 新闻分类（体育、财经、科技等）
- 垃圾邮件过滤
- 意图识别（客服系统）

**示例：**
```
"A股今日大涨，沪指上涨3.2%" → 财经类
"梅西打进致胜一球" → 体育类
```

#### （3）问答系统（Question Answering）
**定义：** 根据问题从知识库或文档中找到准确答案。

**类型：**
- **抽取式问答**：从原文中抽取答案片段
- **生成式问答**：生成自然语言答案
- **知识库问答**：基于结构化知识库回答

**示例：**
```
问题："中国的首都是哪里？"
答案："北京"
```

#### （4）阅读理解（Reading Comprehension）
**定义：** 理解给定文章，回答相关问题。

**经典数据集：**
- SQuAD（Stanford Question Answering Dataset）
- CMRC（中文阅读理解数据集）

### 1.2.3 生成类任务

#### （1）机器翻译（Machine Translation）
**定义：** 将一种语言的文本自动翻译成另一种语言。

**发展阶段：**
- 规则机器翻译（RBMT）
- 统计机器翻译（SMT）
- 神经机器翻译（NMT）← 当前主流

**示例：**
```
英文："Natural Language Processing is fascinating."
中文："自然语言处理很迷人。"
```

#### （2）文本摘要（Text Summarization）
**定义：** 从原文中提取或生成简洁的摘要。

**类型：**
- **抽取式摘要**：选择原文中的关键句子
- **生成式摘要**：重新生成概括性文本

**应用：** 新闻摘要、论文摘要、会议纪要生成

#### （3）对话系统（Dialogue System）
**定义：** 使计算机能够与人类进行自然对话。

**类型：**
- **任务型对话**：完成特定任务（如订票、查天气）
- **闲聊型对话**：开放域聊天
- **问答型对话**：基于知识的问答

**代表系统：** Siri、小爱同学、ChatGPT

#### （4）文本生成（Text Generation）
**定义：** 根据输入条件生成连贯的自然语言文本。

**应用场景：**
- 自动写作（新闻稿、广告文案）
- 诗歌生成
- 代码注释生成
- 数据到文本（Data-to-Text）

### 1.2.4 其他重要任务

#### （1）信息抽取（Information Extraction）
从非结构化文本中提取结构化信息（实体、关系、事件）。

#### （2）文本相似度计算
判断两段文本的语义相似程度，用于：
- 问题匹配
- 文本去重
- 推荐系统

#### （3）关键词提取
从文档中提取最重要的词汇或短语。

---

## 1.3 技术演进历史

### 1.3.1 早期阶段：规则与符号方法（1950s-1980s）

#### **时代背景**
- 计算机科学刚刚起步
- 语言学理论占主导地位
- 计算资源极其有限

#### **主要特点**
- **基于规则的方法**：由语言学家手工编写语法规则
- **符号主义**：使用逻辑符号表示语言知识
- **专家系统**：依赖领域专家的知识

#### **代表性工作**
- **1950年** - 图灵测试提出
- **1954年** - Georgetown-IBM实验（首个机器翻译系统）
- **1960年代** - ELIZA聊天机器人（首个对话系统）

#### **局限性**
- 规则覆盖面有限，难以处理语言的多样性
- 维护成本高，扩展性差
- 无法处理歧义和例外情况

### 1.3.2 统计方法时代（1990s-2010s）

#### **时代背景**
- 大规模语料库出现
- 计算能力提升
- 机器学习理论发展

#### **核心思想**
从大量文本数据中学习统计规律，而非手工编写规则。

#### **关键技术**

**（1）N-gram语言模型**
- 基于前N-1个词预测下一个词
- 应用于语音识别、机器翻译

**（2）隐马尔可夫模型（HMM）**
- 用于词性标注、命名实体识别
- 基于状态转移概率

**（3）条件随机场（CRF）**
- 序列标注任务的利器
- 考虑上下文依赖关系

**（4）统计机器翻译（SMT）**
- 基于短语的翻译模型
- Google Translate早期版本

#### **代表性成果**
- **IBM翻译模型**（1990s）
- **Penn Treebank**语料库
- **WordNet**语义网络

#### **优势**
- 自动从数据中学习，无需手工规则
- 对新领域适应性强
- 性能显著超越规则方法

#### **局限性**
- 需要大量标注数据
- 难以捕捉长距离依赖关系
- 语义理解能力有限

### 1.3.3 深度学习时代（2013-2017）

#### **时代背景**
- GPU算力爆发
- 大规模数据集积累
- 深度学习在CV领域的成功

#### **核心突破**

**（1）词嵌入（Word Embedding）**
- **Word2Vec**（2013）- Mikolov et al.
  - 将词映射到连续向量空间
  - 相似词的向量距离接近
  - "king - man + woman ≈ queen"

- **GloVe**（2014）- Stanford
  - 结合全局统计信息

**（2）循环神经网络（RNN/LSTM）**
- **LSTM**（1997年提出，2013年后流行）
  - 解决长距离依赖问题
  - 用于序列建模

- **GRU**（2014）
  - LSTM的简化版本

- **Seq2Seq**（2014）
  - 编码器-解码器架构
  - 机器翻译性能飞跃

**（3）注意力机制（Attention）**
- **2015年** - Bahdanau Attention
  - 让模型"关注"输入的关键部分
  - 显著提升翻译质量

#### **影响**
- NLP任务性能大幅提升
- 端到端学习成为主流
- 特征工程需求减少

### 1.3.4 Transformer与预训练时代（2017-至今）

#### **第一阶段：Transformer革命（2017-2018）**

**里程碑论文：**
- **"Attention is All You Need"**（2017）
  - Google提出Transformer架构
  - 完全抛弃RNN，纯基于注意力机制
  - 并行计算能力强，训练速度快

**Transformer核心组件：**
- 自注意力机制（Self-Attention）
- 多头注意力（Multi-Head Attention）
- 位置编码（Positional Encoding）
- 前馈神经网络

#### **第二阶段：预训练模型爆发（2018-2020）**

**代表性模型：**

**（1）ELMo（2018）**
- 首个上下文相关的词向量
- 双向LSTM结构

**（2）GPT（2018）- OpenAI**
- 单向Transformer语言模型
- 预训练+微调范式

**（3）BERT（2018）- Google**
- **双向Transformer编码器**
- Masked Language Model（MLM）
- Next Sentence Prediction（NSP）
- 在11个NLP任务上刷新SOTA

**（4）GPT-2（2019）**
- 15亿参数
- Zero-shot学习能力

**（5）RoBERTa、ALBERT、ELECTRA（2019-2020）**
- BERT的改进版本

#### **第三阶段：大语言模型（LLM）时代（2020-至今）**

**超大规模模型：**

**（1）GPT-3（2020）**
- 1750亿参数
- Few-shot学习
- 涌现能力（Emergent Abilities）

**（2）T5、BART（2020）**
- 统一的Text-to-Text框架

**（3）ChatGPT（2022.11）**
- 基于GPT-3.5
- RLHF（人类反馈强化学习）
- 现象级应用，引爆AI革命

**（4）GPT-4（2023.3）**
- 多模态能力
- 更强的推理能力

**（5）开源大模型崛起（2023-至今）**
- **LLaMA**（Meta）
- **LLaMA 2/3**（开源商用）
- **ChatGLM**（清华）
- **Qwen**（阿里）
- **Baichuan**（百川智能）

#### **当前趋势**

**（1）模型规模持续增长**
- 从亿级参数到千亿级
- 涌现能力不断增强

**（2）对齐技术（Alignment）**
- RLHF（强化学习人类反馈）
- DPO、PPO等优化算法
- 让模型更符合人类价值观

**（3）高效训练与微调**
- LoRA、QLoRA
- Prefix Tuning、P-Tuning
- 降低微调成本

**（4）多模态融合**
- 图文多模态（CLIP、LLaVA）
- 视频理解、音频处理

**（5）Agent与工具使用**
- 模型调用外部工具
- 自主规划和执行任务

**（6）长文本处理**
- 从2K上下文到100K+
- Flash Attention技术

### 1.3.5 技术演进总结

```
时间线视图：
1950s-1980s  → 规则符号时代    [手工规则]
1990s-2010s  → 统计学习时代    [机器学习]
2013-2017    → 深度学习时代    [神经网络]
2017-2020    → Transformer时代 [预训练]
2020-至今    → 大模型时代      [涌现智能]
```

**核心演进逻辑：**
1. **数据驱动**：从手工规则→统计学习→深度学习
2. **表示学习**：离散符号→稀疏特征→密集向量→上下文表示
3. **模型能力**：专用模型→通用模型→超大规模模型
4. **学习范式**：有监督学习→预训练+微调→Prompt学习

---

## 本章小结

1. **NLP定义**：让计算机理解和生成人类语言的技术
2. **核心任务**：涵盖理解、生成、分析等多个层面
3. **技术演进**：从规则→统计→深度学习→大模型
4. **当前前沿**：大语言模型（LLM）正在重新定义NLP
