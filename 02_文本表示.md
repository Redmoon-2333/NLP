# 第2章 文本表示

文本表示（Text Representation）是自然语言处理的核心基础，它将人类可读的文本转换为计算机可以处理的数学形式。文本表示的质量直接影响下游NLP任务的性能。

---

## 2.1 概述

### 2.1.1 什么是文本表示

**文本表示**是指将自然语言文本（词、句子、文档）映射到数学空间（通常是向量空间）的过程，使得计算机能够进行数值计算和模式识别。

**核心目标：**
- 将离散的文本符号转换为连续的数值向量
- 捕捉文本的语义信息和语法结构
- 支持文本之间的相似度计算和关系推理

### 2.1.2 文本表示的重要性

**为什么需要文本表示？**

1. **计算机只能处理数字**
   - 机器学习算法需要数值输入
   - 无法直接对文字进行数学运算

2. **语义信息的数学化**
   - 将"相似"的词映射到向量空间中接近的位置
   - 支持语义关系的数学建模

3. **下游任务的基础**
   - 文本分类、情感分析、机器翻译等任务都依赖文本表示
   - 好的表示能显著提升模型性能

### 2.1.3 文本表示的发展历程

```
发展时间线：
1950s-1990s  → 符号表示（One-Hot、词袋模型）
2000s        → 统计表示（TF-IDF、LSA）
2013-2017    → 静态词向量（Word2Vec、GloVe）
2018-至今    → 上下文词向量（ELMo、BERT、GPT）
```

**演进趋势：**
- 从离散到连续
- 从稀疏到密集
- 从静态到动态
- 从词级到句级、篇章级

---

## 2.2 分词

分词（Word Segmentation / Tokenization）是文本表示的第一步，将连续的文本序列切分成有意义的最小单元（词或子词）。

### 2.2.1 分词的必要性

**为什么需要分词？**

1. **词是语义的基本单位**
   - 词是承载独立语义的最小单位
   - 便于后续的特征提取和模型训练

2. **降低计算复杂度**
   - 相比字符级，词级表示维度更低
   - 减少序列长度，提高计算效率

3. **跨语言差异**
   - 英文等拉丁语系：天然以空格分隔
   - 中文、日文等：无明显分隔符，需要算法识别

**初学者常见疑问：**

**Q1：为什么不能按字符处理中文？**
A：
```
按字符处理的问题：
“我 爱 自 然 语 言 处 理” → 8个字符
问题：
1. 序列太长，计算量大
2. 语义信息丢失：“自然语言”是一个完整概念，被拆成四个字
3. 词汇表巨大：常用汉字约3000个，但常用词汇更多

按词处理的好处：
“我 / 爱 / 自然语言 / 处理” → 4个Token
优势：
1. 序列更短
2. 保留完整语义
3. 更符合人类理解
```

**Q2：英文为什么也需要分词？不是有空格吗？**
A：
```
简单的split()不够：

原文："Don't you think it's amazing?"

直接split()：["Don't", "you", "think", "it's", "amazing?"]
问题：
- 标点符号没分开："amazing?"
- 缩写没处理："Don't" 应该是 "Do" + "n't"

正确分词：["Do", "n't", "you", "think", "it", "'s", "amazing", "?"]
好处：
- 缩写正确处理
- 标点单独分离
- 便于后续处理
```

**Q3：分词结果为什么不统一？**
A：
```
同一句话的不同分法：

原文：“研究生命起源”

分法1：[“研究生”, “命”, “起源”]  # “研究生”作为身份
分法2：[“研究”, “生命”, “起源”]  # “研究”作为动词

原因：
1. 不同的分词工具有不同的词典和算法
2. 不同的应用场景有不同的需求
3. 中文语言本身就存在歧义

建议：根据任务选择合适的分词工具，保持一致性
```

### 2.2.2 中文分词方法

#### **（1）基于规则的方法**

**正向最大匹配（FMM - Forward Maximum Matching）**

**原理：**
- 从左向右扫描，每次匹配词典中最长的词

**示例：**
```
句子："南京市长江大桥"
词典：["南京", "南京市", "市长", "长江", "大桥", "长江大桥"]

正向最大匹配：
1. "南京市" ✓（最长匹配）
2. "长江大桥" ✓
结果：["南京市", "长江大桥"]
```

**逆向最大匹配（BMM - Backward Maximum Matching）**
- 从右向左扫描

**双向最大匹配**
- 结合正向和逆向结果，选择分词数量少的

**优缺点：**
- ✅ 简单高效，无需训练
- ❌ 依赖词典质量，无法处理未登录词（OOV）
- ❌ 歧义处理能力弱

#### **（2）基于统计的方法**

**隐马尔可夫模型（HMM）**

**核心思想：**
- 将分词视为序列标注问题
- 为每个字标注位置标签：B（开始）、M（中间）、E（结束）、S（单字词）

**示例：**
```
输入："我爱自然语言处理"
标注：我/S 爱/S 自/B 然/M 语/M 言/E 处/B 理/E
分词：["我", "爱", "自然语言", "处理"]
```

**条件随机场（CRF）**

**优势：**
- 考虑全局特征，解决HMM的标注偏置问题
- 当前主流统计方法之一

**特点：**
- ✅ 可以学习未登录词
- ✅ 考虑上下文信息
- ❌ 需要大量标注数据

#### **（3）基于深度学习的方法**

**Bi-LSTM + CRF**

**架构：**
```
输入字符序列
    ↓
字符嵌入层
    ↓
双向LSTM层（捕捉上下文）
    ↓
CRF层（全局最优标注序列）
    ↓
BMES标签序列
```

**优势：**
- 自动学习特征，无需人工特征工程
- 性能优于传统统计方法

**Transformer分词模型**

**代表工作：**
- BERT的中文分词
- 使用字符级BERT + CRF

### 2.2.3 英文分词（Tokenization）

#### **（1）基于空格和标点的分词**

**简单方法：**
```python
text = "Hello, world! How are you?"
tokens = text.split()  # ['Hello,', 'world!', 'How', 'are', 'you?']
```

**问题：**
- 标点符号未分离
- 无法处理缩写（don't → do + n't）

#### **（2）基于规则的高级分词**

**NLTK Tokenizer**
```python
from nltk.tokenize import word_tokenize

text = "Don't you think it's amazing?"
tokens = word_tokenize(text)
# ['Do', "n't", 'you', 'think', 'it', "'s", 'amazing', '?']
```

**spaCy Tokenizer**
- 基于规则 + 异常词典
- 处理缩写、连字符等复杂情况

#### **（3）子词分词（Subword Tokenization）**

现代NLP模型的主流方法，平衡词汇表大小和覆盖率。

**BPE（Byte Pair Encoding）**

**原理：**
1. 从字符级开始
2. 迭代合并最频繁的字符对
3. 直到达到目标词汇表大小

**示例：**
```
初始词汇表：['l', 'o', 'w', 'e', 's', 't']
语料："low", "lower", "newest", "widest"

迭代过程：
1. 合并 'e'+'s' → 'es' (highest frequency)
2. 合并 'es'+'t' → 'est'
3. 合并 'l'+'o' → 'lo'
4. 合并 'lo'+'w' → 'low'

最终："low", "low er", "new est", "wid est"
```

**应用：** GPT系列模型

**WordPiece**

**原理：**
- 与BPE类似，但选择合并对时基于语言模型概率
- 选择能最大化训练数据似然的合并

**示例：**
```
"playing" → ["play", "##ing"]
"unaffable" → ["un", "##aff", "##able"]
```

**应用：** BERT、DistilBERT

**SentencePiece**

**特点：**
- 直接从原始文本训练，无需预分词
- 将空格也视为特殊字符
- 支持BPE和Unigram两种算法

**应用：** T5、XLNet、ALBERT

**优势总结：**
- ✅ 有效处理未登录词（OOV）
- ✅ 减小词汇表大小
- ✅ 捕捉词根和词缀信息
- ✅ 多语言友好

**初学者理解：子词分词 vs 词级分词**

**为什么需要子词分词？**

1. **解决未登录词问题**
   ```
   场景：遇到一个新词 "unbelievable"
   
   词级分词：
   - 如果词典没有这个词 → [UNK] (未知)
   - 丢失信息，模型无法理解
   
   子词分词 (BPE/WordPiece):
   - "unbelievable" → ["un", "##believ", "##able"]
   - 保留了词缀信息：un-(否定), -able(能够)
   - 模型仍然能理解大概意思
   ```

2. **平衡词汇表大小和覆盖率**
   ```
   词级分词的问题：
   - 词汇表太小 → 很多[UNK]
   - 词汇表太大 → 计算量太大，参数太多
   
   子词分词的优势：
   词汇表示例：30,000个子词
   - 可以组合出无数个词
   - 几乎没有[UNK]
   - 计算量可控
   ```

3. **捕捉词法结构**
   ```
   英文示例：
   - "play", "playing", "played", "player"
   
   词级分词：
   - 4个完全不同的词，占用4个词汇表位置
   - 模型难以学习它们的关系
   
   子词分词：
   - "play", "play" + "##ing", "play" + "##ed", "play" + "##er"
   - 共享 "play" 这个基础子词
   - 模型轻松学习词根和词缀的关系
   ```

**三种分词方法对比：**

| 特性 | 字符级 | 词级 | 子词级 |
|------|---------|-------|----------|
| **词汇表大小** | 小 (~100) | 大 (50K-500K) | 中 (30K-50K) |
| **OOV问题** | 无 | 严重 | 基本解决 |
| **序列长度** | 长 | 短 | 中 |
| **计算效率** | 低 | 高 | 中 |
| **语义完整性** | 差 | 好 | 中 |
| **适用场景** | 中文字符 | 传统任务 | **现代大模型** |

**实际示例对比：**

```python
# 原始文本
text = "I love natural language processing!"

# 方法1：字符级
# ['I', ' ', 'l', 'o', 'v', 'e', ' ', 'n', 'a', 't', 'u', 'r', 'a', 'l', ...]
# 序列长度：39个字符

# 方法2：词级
# ['I', 'love', 'natural', 'language', 'processing', '!']
# 序列长度：6个token

# 方法3：子词级 (WordPiece)
# ['I', 'love', 'natural', 'language', 'process', '##ing', '!']
# 序列长度：7个token
```

**BERT、GPT为什么使用子词分词？**

```
现代大模型的需求：

1. **多语言支持**
   - 一个模型需要处理几十种语言
   - 词级分词：每种语言都需要单独的词典 → 不现实
   - 子词分词：直接从原始文本学习 → 通用性强

2. **处理新词、专有名词**
   - 互联网时代新词层出不穷："ChatGPT", "TikTok"
   - 子词分词可以将其拆分，保留部分信息

3. **降低训练成本**
   - 词汇表越大，模型参数越多
   - 子词平衡了词汇表大小和表达能力
```

### 2.2.4 常用分词工具

#### **中文分词工具**

**（1）jieba（结巴分词）**

**特点：**
- 支持三种分词模式：精确模式、全模式、搜索引擎模式
- 支持自定义词典
- 支持词性标注

**使用示例：**

```python
import jieba

# 定义测试文本
text = "小明毕业于北京大学计算机系"

# 1. 精确模式（默认模式）
# 试图将句子最精确地切分，适合文本分析
# lcut() 返回list类型的分词结果
jieba.lcut(text)
# 输出: ['小明', '毕业', '于', '北京大学', '计算机系']

# 2. 全模式
# 扫描出句子中所有可能的词，速度快但存在冗余
# cut_all=True 启用全模式
jieba.lcut(text, cut_all=True)
# 输出: ['小', '明', '毕业', '于', '北京', '北京大学', '大学', '计算', '计算机', '计算机系', '算机', '系']

# 3. 搜索引擎模式
# 在精确模式基础上，对长词再次切分，提高召回率
jieba.lcut_for_search(text)
# 输出: ['小明', '毕业', '于', '北京', '大学', '北京大学', '计算', '算机', '计算机', '计算机系']
```

**自定义词典使用：**

```python
# 加载自定义词典，用于识别特定领域词汇
# 词典文件格式：词语 词频 词性
jieba.load_userdict("./data/user_dict.txt")

# 使用自定义词典后的分词效果
# 专业词汇如'云计算'、'云原生'、'大模型'被正确识别
text = "随着云计算技术的普及，越来越多企业开始采用云原生架构"
jieba.lcut(text)
```

**（2）pkuseg**

**特点：**
- 北京大学开源
- 支持多领域分词（新闻、医疗、旅游等）
- 在某些领域准确率高于jieba

**（3）LTP（语言技术平台）**

**特点：**
- 哈工大开源
- 提供完整的NLP工具链（分词、词性标注、命名实体识别等）
- 学术研究常用

**（4）HanLP**

**特点：**
- 功能全面的NLP工具包
- 支持多种分词算法
- 工业级性能

#### **英文分词工具**

**（1）NLTK**
```python
from nltk.tokenize import word_tokenize, sent_tokenize

# 句子分割
text = "Hello world. How are you?"
sentences = sent_tokenize(text)

# 词分割
words = word_tokenize(text)
```

**（2）spaCy**
```python
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Apple is looking at buying U.K. startup for $1 billion")

for token in doc:
    print(token.text, token.pos_, token.dep_)
```

**（3）Hugging Face Tokenizers**
```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
tokens = tokenizer.tokenize("Hello, how are you?")
# ['hello', ',', 'how', 'are', 'you', '?']
```

### 2.2.5 分词评估指标

**准确率（Precision）**
```
P = 正确分词数 / 系统分词总数
```

**召回率（Recall）**
```
R = 正确分词数 / 标准答案分词总数
```

**F1值**
```
F1 = 2 × (P × R) / (P + R)
```

---

# 2.3 词表示

词表示（Word Representation）将词映射到向量空间，是文本表示的核心内容。

### 2.3.0 基础概念详解（入门必读）

在深入学习各种词表示方法之前，我们需要理解几个基础概念，这些概念贯穿整个NLP领域。

#### **（1）Token（词元）**

**通俗解释：**
Token是文本处理的最小单位，可以理解为"文本被切分后的小块"。

**Token可以是：**
- **完整的词**："我"、"爱"、"自然语言处理"
- **子词**："##ing"、"un##"（前缀/后缀）
- **字符**："a"、"我"
- **特殊符号**："[CLS]"、"[SEP]"、"[PAD]"

**示例：**
```
原始文本："我想吃"

分词后的Token序列：
方式1（词级）：["我", "想", "吃"]
方式2（字符级）：["我", "想", "吃"]
方式3（子词级）：["我", "想", "吃"]  # 中文字符级和词级常常一致

英文例子："playing"
词级Token：["playing"]
子词Token：["play", "##ing"]  # WordPiece分词
```

**为什么叫Token？**
- "Token"原意是"令牌"、"标记"，表示文本被标记分解后的单元

#### **（2）词表（Vocabulary）**

**通俗解释：**
词表就像一本字典，记录了模型认识的所有Token及其对应的编号（ID）。

**词表的结构：**
```
词表示例（简化版）：

Token     →  ID（索引）
─────────────────────
[PAD]     →  0      # 填充符号
[UNK]     →  1      # 未知词
[CLS]     →  2      # 句子开始
[SEP]     →  3      # 句子分隔
我        →  4
爱        →  5
你        →  6
想        →  7
北京      →  8
吃        →  9
很        →  10
好        →  11
去        →  12
...       →  ...
```

**词表的作用：**
1. **文本→数字**：将Token转换为ID
   ```
   "我想去北京" → [4, 7, 12, 8]
   ```

2. **数字→文本**：将ID还原为Token
   ```
   [5, 6] → "爱你"
   ```

**词表大小的影响：**
- **太小**：很多词变成[UNK]（未知），丢失信息
- **太大**：计算量大，训练慢
- **常见大小**：BERT中文（21,128个），GPT-2英文（50,257个）

#### **（3）嵌入层（Embedding Layer）**

**通俗解释：**
嵌入层是一个"查找表"，将每个Token的ID转换为一个向量（一串数字）。

**为什么需要嵌入层？**
```
问题：ID只是编号，没有语义信息
ID=4（我）和ID=5（爱）之间没有关系体现

解决：通过嵌入层，将每个ID映射到高维空间的一个点（向量）
相似的词在向量空间中距离更近
```

**嵌入层的工作过程（图片中的示例）：**

```
步骤1：分词
输入文本："我想"
分词结果：["我", "想"]

步骤2：Token转ID（查词表）
"我" → ID=0
"想" → ID=3
Token序列变为：[0, 3]

步骤3：ID通过嵌入层转为向量（查嵌入矩阵）
嵌入矩阵（Embedding Matrix）：
┌────────────────────────────┐
│  ID  │  向量表示          │
├──────┼────────────────────┤
│  0   │ [1.2, 3.3, 3.2, 2.1] │  ← "我"的向量
│  1   │ [2.1, 6.5, 1.2, 6.8] │
│  2   │ [5.2, 0.8, 9.5, 1.6] │
│  3   │ [1.8, 0.9, 0.6, 2.2] │  ← "想"的向量
│  4   │ [5.1, 2.7, 1.9, 7.3] │
│ ...  │      ...            │
└────────────────────────────┘

输出向量序列：
"我" (ID=0) → [1.2, 3.3, 3.2, 2.1]
"想" (ID=3) → [1.8, 0.9, 0.6, 2.2]
```

**嵌入层的维度：**
- **常见维度**：128、256、512、768、1024
- **BERT-base**：768维
- **GPT-3**：12,288维

**维度的含义：**
- 每个维度可以理解为词的某种"特征"
- 例如：第1维可能表示"是否为名词"，第2维表示"情感倾向"等
- 实际上模型自动学习这些特征，人类难以直接解释

#### **（4）词向量（Word Vector / Word Embedding）**

**通俗解释：**
词向量就是嵌入层输出的那个向量，它用一串数字表示一个词。

**为什么叫"向量"？**
```
数学上，向量是有方向和大小的量
词向量在高维空间中的位置和方向蕴含了词的语义信息

例如：
vec("国王") = [0.5, 0.8, 0.3, ...]
vec("女王") = [0.5, 0.7, 0.4, ...]  # 向量相近，语义相似
vec("桌子") = [0.1, 0.2, 0.9, ...]  # 向量远离，语义不同
```

**词向量的神奇性质（向量运算）：**
```python
# 语义相似性
vec("国王") 与 vec("皇帝") 的余弦距离很小（相似）
vec("国王") 与 vec("桌子") 的余弦距离很大（不相似）

# 类比关系（向量代数）
vec("国王") - vec("男人") + vec("女人") ≈ vec("女王")
解释：国王去掉"男性"特征，加上"女性"特征 = 女王

vec("中国") - vec("北京") + vec("东京") ≈ vec("日本")
解释：国家与首都的关系
```

#### **（5）完整流程示例（结合图片）**

**场景：语言模型预测下一个词**

```
输入："我想"
目标：预测下一个词

┌─────────────────────────────────────────────────────────────┐
│  第1步：分词                                                  │
│  "我想" → ["我", "想"]                                      │
└─────────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────────┐
│  第2步：Token转ID（查词表）                                   │
│  "我" → 0                                                    │
│  "想" → 3                                                    │
│  序列：[0, 3]                                                │
└─────────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────────┐
│  第3步：ID通过嵌入层转为向量                                  │
│  ┌─────────────────┐                                        │
│  │  嵌入层矩阵      │                                        │
│  │ (Embedding)     │                                        │
│  ├─────────────────┤                                        │
│  │ 0 → [1.2, 3.3,  │ ← "我"的词向量                         │
│  │      3.2, 2.1]  │                                        │
│  │ 1 → [2.1, 6.5,  │                                        │
│  │      1.2, 6.8]  │                                        │
│  │ 2 → [5.2, 0.8,  │                                        │
│  │      9.5, 1.6]  │                                        │
│  │ 3 → [1.8, 0.9,  │ ← "想"的词向量                         │
│  │      0.6, 2.2]  │                                        │
│  │ ... → ...       │                                        │
│  └─────────────────┘                                        │
└─────────────────────────────────────────────────────────────┘
               zh          ↓
┌─────────────────────────────────────────────────────────────┐
│  第4步：词向量输入到神经网络（隐藏层）                         │
│  神经网络处理这些向量，理解上下文语义                          │
└─────────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────────┐
│  第5步：输出层预测下一个Token的概率分布                        │
│  ┌──────────────┐                                           │
│  │ 输出层        │                                           │
│  ├──────────────┤                                           │
│  │ 0 → "我"     │ 概率: 0.01                                │
│  │ 1 → "爱"     │ 概率: 0.02                                │
│  │ 2 → "你"     │ 概率: 0.03                                │
│  │ 3 → "想"     │ 概率: 0.01                                │
│  │ 4 → "北京"   │ 概率: 0.35  ← 最高概率                    │
│  │ 5 → "吃"     │ 概率: 0.23                                │
│  │ 6 → "烤鸭"   │ 概率: 0.01                                │
│  │ 7 → "去"     │ 概率: 0.33                                │
│  │ ... → ...    │ ...                                       │
│  └──────────────┘                                           │
└─────────────────────────────────────────────────────────────┘
                         ↓
┌─────────────────────────────────────────────────────────────┐
│  第6步：选择概率最高的Token                                   │
│  预测结果："北京"（概率0.35最高）                            │
│  完整句子："我想北京"                                        │
└─────────────────────────────────────────────────────────────┘
```

#### **（6）关键概念对比总结**

| 概念 | 是什么 | 示例 | 作用 |
|------|--------|------|------|
| **Token** | 文本的最小单位 | "我"、"##ing" | 分词后的结果 |
| **词表** | Token到ID的映射表 | {"我":0, "爱":1} | 文本数字化 |
| **ID** | Token的编号 | 0, 1, 2, 3... | 便于计算机索引 |
| **嵌入层** | ID到向量的转换层 | 查找表/矩阵 | 将离散ID变为连续向量 |
| **词向量** | 词的向量表示 | [1.2, 3.3, 2.1] | 包含语义信息的数值 |

#### **（7）初学者常见疑问**

**Q1: 词向量是怎么得到的？**
A: 有两种方式：
1. **随机初始化后训练**：刚开始是随机数，通过训练任务（如预测下一个词）不断调整
2. **使用预训练词向量**：直接加载Word2Vec、GloVe等已经训练好的词向量

**Q2: 为什么不直接用ID，要转成向量？**
A: 
- ID只是编号，ID=5和ID=6之间没有关系
- 词向量用连续的数值表示，相似的词向量接近，模型可以学习语义关系
- 例如："猫"和"狗"的ID可能差很远，但它们的词向量应该接近（都是动物）

**Q3: 嵌入层的参数从哪来？**
A: 
- 嵌入层本质是一个可学习的矩阵（参数）
- 训练时，这个矩阵的值会不断更新优化
- 目标是让相似的词在向量空间中距离更近

**Q4: 同一个词在不同句子中的词向量一样吗？**
A: 
- **静态词向量**（Word2Vec、GloVe）：一样，一个词只有一个固定向量
- **上下文词向量**（BERT、GPT）：不一样，根据上下文动态生成
  ```
  "苹果很好吃"中的"苹果" vs "苹果公司"中的"苹果"
  BERT会给出不同的向量（前者是水果，后者是公司）
  ```

---

### 2.3.1 离散表示

离散表示将词视为独立的符号，不考虑词之间的语义关系。

#### **（1）One-Hot编码**

**定义：**
- 词汇表大小V，每个词表示为V维向量
- 该词对应位置为1，其余为0

**通俗理解：**

One-Hot编码就像给每个词分配一个“座位号”，只有这个词自己的位置是1，其他位置都是0。

**形象比喻：**
```
想象一个教室有5个座位，5个学生：

座位：  [1号位] [2号位] [3号位] [4号位] [5号位]

小明坐在1号： [1,     0,     0,     0,     0]  ← 只有1号位有人
小红坐在2号： [0,     1,     0,     0,     0]  ← 只有2号位有人
小刚坐在3号： [0,     0,     1,     0,     0]  ← 只有3号位有人

同理，每个词在词汇表中有自己的“座位”
```

**示例：**
```
词汇表：["我", "爱", "自然", "语言", "处理"]
词汇表大小 V = 5

One-Hot表示：
         位置: 0  1  2  3  4
我:     [1, 0, 0, 0, 0]  ← 在位置0
爱:     [0, 1, 0, 0, 0]  ← 在位置1
自然:   [0, 0, 1, 0, 0]  ← 在位置2
语言:   [0, 0, 0, 1, 0]  ← 在位置3
处理:   [0, 0, 0, 0, 1]  ← 在位置4
```

**Python实现：**

```python
import numpy as np

# 词汇表
vocab = ["我", "爱", "自然", "语言", "处理"]
vocab_size = len(vocab)

# 为词"爱"生成One-Hot向量
word = "爱"
word_idx = vocab.index(word)  # 获取索引 = 1

# 创建全0向量，然后将对应位置设为1
one_hot = np.zeros(vocab_size)
one_hot[word_idx] = 1

print(f"'爱'的One-Hot: {one_hot}")
# 输出: [0. 1. 0. 0. 0.]
```

**优点：**
- ✅ 简单直观，易于实现
- ✅ 不同词之间完全独立（向量正交）

**缺点：**
- ❌ **维度灾难**：词汇表通常有几万到几十万，导致极高维度
  ```
  实际场景：
  中文词汇表 = 50,000个词
  每个词的One-Hot向量 = 50,000维
  存储一句话（10个词） = 50,000 × 10 = 500,000个数字！
  ```

- ❌ **稀疏性**：每个向量只有一个1，其余全是0
  ```
  50,000维向量中，只有1个有用信息，其他49,999个都是0
  浪费大量存储空间和计算资源
  ```

- ❌ **无法捕捉语义**：任意两个词的向量正交，无法表达相似性
  ```python
  vec("国王") = [1, 0, 0, 0, 0]
  vec("皇帝") = [0, 1, 0, 0, 0]
  vec("桌子") = [0, 0, 1, 0, 0]
  
  # 计算余弦相似度
  cos("国王", "皇帝") = 0  # 完全不相似？但它们语义接近！
  cos("国王", "桌子") = 0  # 也是完全不相似？这才是对的！
  
  问题：One-Hot无法区分"相似"和"不相似"的词
  ```

- ❌ **无法处理未登录词**
  ```
  如果词汇表没有某个词，就无法表示
  例如：新出现的网络用语 "yyds" → 无法表示
  ```

**为什么还要学One-Hot？**

虽然One-Hot有很多缺点，但它是理解词表示的基础：
1. **历史重要性**：是最早的词表示方法
2. **理解后续技术**：词向量、嵌入层都是为了解决One-Hot的问题
3. **仍有应用**：在某些简单场景（如小词汇表分类）仍然使用

**数学表示：**
```
词 w_i 的 One-Hot 向量：e_i ∈ ℝ^V
其中 e_i[j] = 1 if j=i, else 0
```

#### **（2）词袋模型（Bag of Words, BoW）**

**定义：**
- 将文本表示为词频向量
- 忽略词序和语法结构，只关注词的出现

**示例：**
```
词汇表：["我", "爱", "自然", "语言", "处理", "学习"]

文档1："我爱自然语言处理"
BoW:   [1, 1, 1, 1, 1, 0]

文档2："我爱学习自然语言处理和机器学习"
BoW:   [1, 1, 1, 1, 1, 2]  # "学习"出现2次
```

**Python实现：**
```python
from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    "我爱自然语言处理",
    "我爱学习自然语言处理"
]

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)
print(vectorizer.get_feature_names_out())
print(X.toarray())
```

**优点：**
- ✅ 简单高效
- ✅ 可用于文本分类等任务

**缺点：**
- ❌ 丢失词序信息
- ❌ 无法捕捉语义
- ❌ 高维稀疏

#### **（3）N-gram模型**

**定义：**
- 考虑连续N个词的组合
- 部分保留词序信息

**示例：**
```
文本："我爱自然语言处理"

1-gram (Unigram): ["我", "爱", "自然", "语言", "处理"]
2-gram (Bigram):  ["我爱", "爱自然", "自然语言", "语言处理"]
3-gram (Trigram): ["我爱自然", "爱自然语言", "自然语言处理"]
```

**应用：**
- 文本分类：捕捉短语特征
- 语言模型：预测下一个词

**缺点：**
- 特征空间爆炸：N越大，特征数指数增长

### 2.3.2 分布式表示（静态词向量）

分布式表示将词映射到低维稠密向量空间，捕捉词的语义信息。

**核心思想：分布式假设（Distributional Hypothesis）**
> "A word is characterized by the company it keeps."  
> —— J.R. Firth (1957)
>
> **相似上下文的词具有相似的语义**

#### **（1）Word2Vec**

**概述：**
- Google 2013年提出
- 两种训练模式：CBOW和Skip-gram
- 浅层神经网络，训练高效

**CBOW（Continuous Bag of Words）**

**目标：**
- 用上下文词预测中心词

**架构：**
```
上下文词 → 输入层 → 隐藏层（求平均） → 输出层 → 中心词
```

**示例：**
```
句子："我 爱 自然 语言 处理"
窗口大小=2

训练样本：
上下文: ["我", "爱", "语言", "处理"] → 目标: "自然"
```

**Skip-gram**

**目标：**
- 用中心词预测上下文词

**架构：**
```
中心词 → 输入层 → 隐藏层 → 输出层 → 上下文词
```

**示例：**
```
句子："我 爱 自然 语言 处理"
窗口大小=2

训练样本：
输入: "自然" → 目标: ["我", "爱", "语言", "处理"]
```

**训练优化：**

1. **负采样（Negative Sampling）**
   - 原始Softmax计算代价高（词汇表大）
   - 负采样：每次只更新少量负样本
   - 将多分类转为二分类

2. **层次Softmax（Hierarchical Softmax）**
   - 使用Huffman树组织词汇表
   - 时间复杂度从O(V)降到O(log V)

**词向量性质：**

**语义相似性：**
```
相似词向量距离近：
vec("国王") ≈ vec("皇帝")
vec("猫") ≈ vec("狗")
```

**语义关系：**
```
vec("国王") - vec("男人") + vec("女人") ≈ vec("女王")
vec("中国") - vec("北京") + vec("东京") ≈ vec("日本")
```

**Python实现：**

```python
import pandas as pd
import jieba
from gensim.models import Word2Vec
from gensim.models import KeyedVectors

# ========== 使用预训练词向量 ==========
# 加载预训练的微博词向量模型（SGNS算法，300维）
model_path = './data/sgns.weibo.word'
model = KeyedVectors.load_word2vec_format(model_path)

# 查看词向量维度
model.vector_size  # 输出: 300

# 查看词表大小
len(model.index_to_key)  # 输出: 195202

# 获取单个词的词向量
model['地铁']  # 返回300维向量

# 计算两个词之间的余弦相似度
# 相似度范围是[-1, 1]，值越大表示语义越相似
model.similarity("公交", "地铁")
# 输出: 0.65458214

# 词向量的类比推理
# 计算公式: vector('男人') + vector('女孩') - vector('男孩')
# 验证 男人-男孩 ≈ 女人-女孩 的语义关系
model.most_similar(positive=['男人', '女孩'], negative=['男孩'])
# 输出: [('女人', 0.6578), ('女孩子', 0.5150), ...]

# ========== 训练自己的Word2Vec模型 ==========
# 加载在线购物评论数据集
df = pd.read_csv('./data/online_shopping_10_cats.csv', encoding='utf-8').dropna()

# 对评论文本进行分词处理
sentences = [[token for token in jieba.lcut(sentence) if token.strip() != ''] 
             for sentence in df['review']]

# 训练Word2Vec模型
w2v_model = Word2Vec(
    sentences,
    vector_size=100,    # 词向量维度
    window=5,           # 上下文窗口大小
    min_count=2,        # 最小词频
    sg=1,               # 1=Skip-gram, 0=CBOW
    workers=4           # 并行线程数
)

# 保存训练好的词向量
w2v_model.wv.save_word2vec_format("./data/word2vec.txt")

# 获取词向量
vector = w2v_model.wv['自然']

# 查找相似词
similar_words = w2v_model.wv.most_similar('自然', topn=5)
```

#### **使用PyTorch的Embedding层加载静态词向量**

PyTorch的Embedding层可以用于加载预训练的静态词向量（如Word2Vec、GloVe），作为神经网络的输入层。

```python
import torch
from torch import nn
from gensim.models import KeyedVectors

# 加载训练好的静态词向量
wv = KeyedVectors.load_word2vec_format('./data/word2vec.txt')

# 处理OOV（Out-of-Vocabulary）问题
unk_token = '<unk>'
index2word = [unk_token] + wv.index_to_key
word2index = {word: index for index, word in enumerate(index2word)}

# 准备词向量矩阵
num_embeddings = len(index2word)
embedding_dim = wv.vector_size
embedding_matrix = torch.randn(num_embeddings, embedding_dim)

for index, word in enumerate(index2word):
    if word in wv:
        embedding_matrix[index] = torch.tensor(wv[word])

# 创建Embedding层（使用预训练的静态词向量初始化）
embedding = nn.Embedding.from_pretrained(embedding_matrix)

# 测试Embedding层
text = "我喜欢乘坐地铁"
tokens = jieba.lcut(text)
input_ids = [word2index.get(token, word2index[unk_token]) for token in tokens]
input_tensor = torch.tensor(input_ids)
output = embedding(input_tensor)
# 输出形状: torch.Size([4, 100]) - 4个词，每个词100维
```

**关键点：**
- `nn.Embedding.from_pretrained()` 使用预训练的**静态词向量**初始化
- 每个词对应唯一的向量表示，不随上下文变化
- 可以选择冻结参数（`freeze=True`）或在训练中微调

---

#### **（2）GloVe（Global Vectors）**

**概述：**
- Stanford 2014年提出
- 结合全局统计信息（词共现矩阵）和局部上下文（Word2Vec）

**核心思想：**
- 词向量的点积应该等于两词的共现概率的对数

**目标函数：**
```
J = Σ f(X_ij) * (w_i^T * w_j + b_i + b_j - log(X_ij))²

其中：
- X_ij：词i和词j的共现次数
- f(X_ij)：权重函数（对高频词降权）
- w_i, w_j：词向量
- b_i, b_j：偏置项
```

**与Word2Vec对比：**

| 特性 | Word2Vec | GloVe |
|------|----------|-------|
| 训练方式 | 局部上下文窗口 | 全局共现矩阵 |
| 计算方式 | 神经网络 | 矩阵分解 |
| 训练速度 | 较快 | 较慢（需构建共现矩阵）|
| 性能 | 在类比任务上稍弱 | 在类比任务上稍强 |

**使用示例：**
```python
# 加载预训练GloVe向量
import numpy as np

def load_glove(file_path):
    embeddings = {}
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            values = line.split()
            word = values[0]
            vector = np.array(values[1:], dtype='float32')
            embeddings[word] = vector
    return embeddings

glove = load_glove('glove.6B.100d.txt')
king_vec = glove['king']
```

#### **（3）FastText**

**概述：**
- Facebook 2017年提出
- Word2Vec的扩展，考虑子词（subword）信息

**核心创新：字符级N-gram**

**原理：**
- 将词拆分为字符N-gram
- 词向量 = 所有N-gram向量的和

**示例：**
```
词："apple" (N=3)
字符N-gram: <ap, app, ppl, ple, le>
（< > 表示词的边界）

vec("apple") = vec(<ap) + vec(app) + vec(ppl) + vec(ple) + vec(le>)
```

**优势：**
- ✅ **处理OOV**：即使词未见过，可以用其子词向量组合
- ✅ **捕捉词法信息**：前缀、后缀、词根等
- ✅ **对形态丰富的语言友好**（如德语、土耳其语）

**示例：**
```python
from gensim.models import FastText

sentences = [["我", "爱", "自然", "语言", "处理"]]

model = FastText(sentences, vector_size=100, window=5, 
                 min_count=1, workers=4, sg=1)

# 即使"自然科学"未在训练集中，也能得到向量
vector = model.wv['自然科学']
```

### 2.3.3 上下文词向量（动态词表示）

静态词向量的局限：**一词多义无法区分**

**示例：**
```
"bank"的两种含义：
1. "I went to the bank to deposit money."（银行）
2. "The river bank was flooded."（河岸）

Word2Vec/GloVe: "bank"只有一个固定向量
```

上下文词向量根据上下文动态生成词的表示。

#### **（1）ELMo（Embeddings from Language Models）**

**概述：**
- 2018年提出
- 基于双向LSTM语言模型
- 首个上下文相关的词向量

**架构：**
```
输入文本
    ↓
字符级CNN（生成初始词表示）
    ↓
双向LSTM（多层）
    ↓
加权组合各层输出
    ↓
上下文词向量
```

**特点：**
- 不同层捕捉不同信息：
  - 底层：句法信息
  - 高层：语义信息
- 根据任务加权组合各层

#### **（2）BERT词向量**

**概述：**
- 基于Transformer编码器
- 双向建模上下文

**使用示例：**

```python
import torch
from transformers import BertTokenizer, BertModel

# 使用BERT获取上下文词向量
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
model = BertModel.from_pretrained('bert-base-chinese')

text = "我爱自然语言处理"
inputs = tokenizer(text, return_tensors='pt')

with torch.no_grad():
    outputs = model(**inputs)
    
# 最后一层隐藏状态作为词向量
# 注意：同一个词在不同上下文中会得到不同的向量表示
embeddings = outputs.last_hidden_state  # [batch_size, seq_len, hidden_dim]
```

**与静态词向量的区别：**
- BERT："苹果"在"苹果很好吃"和"苹果公司"中会得到**不同**的向量
- Word2Vec："苹果"在任何上下文中都是**相同**的向量

**优势：**
- ✅ 真正的双向上下文建模
- ✅ 预训练 + 微调范式
- ✅ 在多个任务上取得SOTA

#### **（3）GPT词向量**

**特点：**
- 单向（从左到右）语言模型
- 适合生成任务

### 2.3.4 词向量评估

**内在评估（Intrinsic Evaluation）**

**（1）词相似度任务**
- 数据集：WordSim-353、SimLex-999
- 指标：Spearman相关系数

**（2）词类比任务**
```
a:b = c:?
king:queen = man:woman
```

**外在评估（Extrinsic Evaluation）**
- 在下游任务（文本分类、NER等）上测试性能

**KeyedVectors常用API速查：**

| 方法 | 功能 | 示例 |
|------|------|------|
| `model[word]` | 获取词向量 | `model['地铁']` |
| `similarity(w1, w2)` | 计算相似度 | `model.similarity('公交', '地铁')` |
| `most_similar(positive, negative)` | 类比推理 | `model.most_similar(positive=['男人','女孩'], negative=['男孩'])` |
| `doesnt_match(words)` | 找出不匹配的词 | `model.doesnt_match(['早餐','午餐','晚餐','电脑'])` |

---

## 本章小结

1. **分词**是文本处理的第一步，中文分词方法包括规则、统计和深度学习方法；现代NLP采用子词分词（BPE、WordPiece）平衡词汇表大小和覆盖率

2. **离散表示**（One-Hot、BoW）简单但高维稀疏，无法捕捉语义

3. **静态词向量**（Word2Vec、GloVe、FastText）实现低维稠密表示，捕捉词的语义相似性和语义关系

4. **上下文词向量**（ELMo、BERT）根据上下文动态生成词表示，解决一词多义问题，是当前主流方法

5. **文本表示的演进**体现了从符号到语义、从静态到动态、从词级到上下文的发展趋势
