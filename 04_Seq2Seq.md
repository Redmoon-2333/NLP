## 4.1 æ¦‚è¿°

### 4.1.1 ä»€ä¹ˆæ˜¯Seq2Seq

**Seq2Seqï¼ˆSequence to Sequenceï¼Œåºåˆ—åˆ°åºåˆ—ï¼‰** æ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œä¸“é—¨ç”¨äºå¤„ç†è¾“å…¥å’Œè¾“å‡ºéƒ½æ˜¯å˜é•¿åºåˆ—çš„ä»»åŠ¡ã€‚å®ƒå°†ä¸€ä¸ªåºåˆ—ï¼ˆå¦‚ä¸€å¥è¯ï¼‰æ˜ å°„åˆ°å¦ä¸€ä¸ªåºåˆ—ï¼ˆå¦‚å¦ä¸€å¥è¯çš„ç¿»è¯‘ï¼‰ã€‚

**æ ¸å¿ƒæ€æƒ³ï¼š**

```
è¾“å…¥åºåˆ—ï¼ˆæºè¯­è¨€ï¼‰        è¾“å‡ºåºåˆ—ï¼ˆç›®æ ‡è¯­è¨€ï¼‰
     â†“                        â†“
"æˆ‘å–œæ¬¢è‡ªç„¶è¯­è¨€å¤„ç†"  â†’  "I like natural language processing"
     â†“                        â†“
[xâ‚, xâ‚‚, xâ‚ƒ, xâ‚„, xâ‚…]  â†’  [yâ‚, yâ‚‚, yâ‚ƒ, yâ‚„, yâ‚…, yâ‚†]
```

**ä¸ºä»€ä¹ˆéœ€è¦Seq2Seqï¼Ÿ**

| ä¼ ç»Ÿæ–¹æ³•çš„é—®é¢˜ | Seq2Seqçš„è§£å†³æ–¹æ¡ˆ |
|---------------|------------------|
| è¾“å…¥è¾“å‡ºé•¿åº¦å¿…é¡»ç›¸åŒ | æ”¯æŒå˜é•¿è¾“å…¥å’Œå˜é•¿è¾“å‡º |
| æ— æ³•å¤„ç†å¤æ‚çš„æ˜ å°„å…³ç³» | é€šè¿‡ç¼–ç å™¨-è§£ç å™¨ç»“æ„å­¦ä¹ å¤æ‚æ˜ å°„ |
| ç¼ºä¹å¯¹ä¸Šä¸‹æ–‡çš„æ•´ä½“ç†è§£ | ç¼–ç å™¨å°†è¾“å…¥å‹ç¼©ä¸ºä¸Šä¸‹æ–‡å‘é‡ï¼Œæ•è·å…¨å±€ä¿¡æ¯ |

### 4.1.2 åº”ç”¨åœºæ™¯

Seq2Seqæ¨¡å‹å¹¿æ³›åº”ç”¨äºä»¥ä¸‹NLPä»»åŠ¡ï¼š

| ä»»åŠ¡ç±»å‹ | è¾“å…¥ç¤ºä¾‹ | è¾“å‡ºç¤ºä¾‹ |
|---------|---------|---------|
| **æœºå™¨ç¿»è¯‘** | "ä½ å¥½ä¸–ç•Œ" | "Hello World" |
| **æ–‡æœ¬æ‘˜è¦** | é•¿ç¯‡æ–‡ç«  | ç®€çŸ­æ‘˜è¦ |
| **å¯¹è¯ç³»ç»Ÿ** | "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ" | "ä»Šå¤©æ™´å¤©ï¼Œæ°”æ¸©25åº¦" |
| **è¯­éŸ³è¯†åˆ«** | éŸ³é¢‘ç‰¹å¾åºåˆ— | æ–‡æœ¬è½¬å½• |
| **ä»£ç ç”Ÿæˆ** | "å†™ä¸€ä¸ªæ’åºå‡½æ•°" | Pythonä»£ç  |
| **é—®ç­”ç³»ç»Ÿ** | "å·´é»æ˜¯å“ªä¸ªå›½å®¶çš„é¦–éƒ½ï¼Ÿ" | "æ³•å›½" |

### 4.1.3 å‘å±•å†ç¨‹

```mermaid
timeline
    title Seq2Seqæ¨¡å‹å‘å±•å†ç¨‹
    2014 : Sutskeverç­‰äººæå‡ºåŸºç¡€Seq2Seqæ¶æ„
         : ä½¿ç”¨LSTMä½œä¸ºç¼–ç å™¨å’Œè§£ç å™¨
    2015 : Bahdanauç­‰äººæå‡ºæ³¨æ„åŠ›æœºåˆ¶
         : è§£å†³é•¿åºåˆ—ä¿¡æ¯ç“¶é¢ˆé—®é¢˜
    2016 : Googleç¥ç»æœºå™¨ç¿»è¯‘ç³»ç»Ÿ(GNMT)
         : å·¥ä¸šçº§Seq2Seqåº”ç”¨
    2017 : Transformeræ¶æ„å‡ºç°
         : å®Œå…¨åŸºäºæ³¨æ„åŠ›ï¼Œå–ä»£RNN
    2019+ : é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ—¶ä»£
          : BERTã€GPTç­‰åŸºäºTransformerçš„æ¨¡å‹
```

---

## 4.2 æ¨¡å‹ç»“æ„è¯¦è§£

### 4.2.1 ç¼–ç å™¨ï¼ˆEncoderï¼‰

**åŠŸèƒ½ï¼š** å°†è¾“å…¥åºåˆ—å‹ç¼©æˆä¸€ä¸ªå›ºå®šé•¿åº¦çš„ä¸Šä¸‹æ–‡å‘é‡ï¼ˆContext Vectorï¼‰ï¼Œæ•è·è¾“å…¥åºåˆ—çš„è¯­ä¹‰ä¿¡æ¯ã€‚

**ç»“æ„ç»„æˆï¼š**

```mermaid
flowchart LR
    subgraph Encoder["ç¼–ç å™¨ Encoder"]
        direction LR
        
        X1(["xâ‚"]) --> E1["Embedding"]
        X2(["xâ‚‚"]) --> E2["Embedding"]
        X3(["xâ‚ƒ"]) --> E3["Embedding"]
        X4(["xâ‚„"]) --> E4["Embedding"]
        
        E1 --> RNN1["RNN/LSTM/GRU"]
        E2 --> RNN2["RNN/LSTM/GRU"]
        E3 --> RNN3["RNN/LSTM/GRU"]
        E4 --> RNN4["RNN/LSTM/GRU"]
        
        RNN1 --> RNN2 --> RNN3 --> RNN4
        
        RNN4 --> C(["ä¸Šä¸‹æ–‡å‘é‡ C"])
    end
    
    style X1 fill:#3498db,stroke:#333,stroke-width:2px,color:#fff
    style X2 fill:#3498db,stroke:#333,stroke-width:2px,color:#fff
    style X3 fill:#3498db,stroke:#333,stroke-width:2px,color:#fff
    style X4 fill:#3498db,stroke:#333,stroke-width:2px,color:#fff
    style C fill:#e74c3c,stroke:#333,stroke-width:2px,color:#fff
```

**å·¥ä½œæµç¨‹ï¼š**

1. **è¯åµŒå…¥ï¼ˆEmbeddingï¼‰**ï¼šå°†è¾“å…¥çš„è¯ç´¢å¼•è½¬æ¢ä¸ºç¨ å¯†å‘é‡
2. **å¾ªç¯ç¼–ç **ï¼šé€šè¿‡RNN/LSTM/GRUé€å±‚å¤„ç†åºåˆ—ï¼Œæ›´æ–°éšè—çŠ¶æ€
3. **æå–ä¸Šä¸‹æ–‡**ï¼šå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ä½œä¸ºä¸Šä¸‹æ–‡å‘é‡

**ä»£ç å®ç°ï¼š**

```python
class TranslationEncoder(nn.Module):
    """ç¿»è¯‘ç¼–ç å™¨ï¼ˆæ„å›¾ï¼šå°†æºè¯­è¨€åºåˆ—ç¼–ç ä¸ºä¸Šä¸‹æ–‡å‘é‡ï¼‰"""
    
    def __init__(self, vocab_size, padding_index):
        super().__init__()
        self.embedding = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=config.EMBEDDING_DIM,
            padding_idx=padding_index  # è­¦ç¤ºï¼špadding_idxç¡®ä¿<pad>æ ‡è®°ä¸å‚ä¸æ¢¯åº¦è®¡ç®—
        )
        self.GRU = nn.GRU(
            input_size=config.EMBEDDING_DIM,
            hidden_size=config.HIDDEN_SIZE,
            batch_first=True  # æ„å›¾ï¼šä½¿ç”¨[batch, seq, feature]æ ¼å¼ï¼Œç¬¦åˆPyTorchä¹ æƒ¯
        )
    
    def forward(self, x):
        """
        å‚æ•°:
            x: [batch_size, seq_len] è¾“å…¥åºåˆ—
        è¿”å›:
            last_hidden_state: [batch_size, hidden_size] æœ€åæ—¶åˆ»éšè—çŠ¶æ€
        """
        embed = self.embedding(x)  # [batch, seq_len, embedding_dim]
        gru_out, hidden = self.GRU(embed)  # gru_out: [batch, seq_len, hidden_size]
        
        # è·å–æ¯ä¸ªåºåˆ—çš„å®é™…é•¿åº¦ï¼ˆæ„å›¾ï¼šå¤„ç†å˜é•¿åºåˆ—ï¼Œå–çœŸå®æœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼‰
        lengths = (x != self.embedding.padding_idx).sum(dim=1)
        last_hidden_state = gru_out[torch.arange(gru_out.shape[0]), lengths - 1]
        
        return last_hidden_state  # [batch, hidden_size]
```

**å…³é”®ç‰¹æ€§ï¼š**

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| ä¿¡æ¯ç“¶é¢ˆ | æ— è®ºè¾“å…¥å¤šé•¿ï¼Œéƒ½å‹ç¼©ä¸ºå›ºå®šç»´åº¦å‘é‡ |
| åŒå‘ç¼–ç  | å¸¸ç”¨åŒå‘RNNæ•è·å®Œæ•´ä¸Šä¸‹æ–‡ |
| å¤šå±‚å †å  | å¤šå±‚ç¼–ç å™¨å­¦ä¹ å±‚æ¬¡åŒ–è¡¨ç¤º |

---

### 4.2.2 è§£ç å™¨ï¼ˆDecoderï¼‰

**åŠŸèƒ½ï¼š** æ ¹æ®ç¼–ç å™¨ç”Ÿæˆçš„ä¸Šä¸‹æ–‡å‘é‡ï¼Œé€æ­¥ç”Ÿæˆè¾“å‡ºåºåˆ—ã€‚

**ç»“æ„ç»„æˆï¼š**

```mermaid
flowchart LR
    subgraph Decoder["è§£ç å™¨ Decoder"]
        direction LR
        
        C(["ä¸Šä¸‹æ–‡å‘é‡ C"]) --> D1["RNN/LSTM/GRU"]
        
        START(["<START>"]) --> E1["Embedding"] --> D1
        D1 --> O1(["yâ‚"])
        D1 --> D2["RNN/LSTM/GRU"
        
        O1 --> E2["Embedding"] --> D2
        D2 --> O2(["yâ‚‚"])
        D2 --> D3["RNN/LSTM/GRU"
        
        O2 --> E3["Embedding"] --> D3
        D3 --> O3(["yâ‚ƒ"])
        D3 --> D4["RNN/LSTM/GRU"
        
        O3 --> E4["Embedding"] --> D4
        D4 --> O4(["<END>"])
    end
    
    style C fill:#e74c3c,stroke:#333,stroke-width:2px,color:#fff
    style START fill:#9b59b6,stroke:#333,stroke-width:2px,color:#fff
    style O1 fill:#2ecc71,stroke:#333,stroke-width:2px
    style O2 fill:#2ecc71,stroke:#333,stroke-width:2px
    style O3 fill:#2ecc71,stroke:#333,stroke-width:2px
    style O4 fill:#f39c12,stroke:#333,stroke-width:2px
```

**å·¥ä½œæµç¨‹ï¼š**

1. **åˆå§‹çŠ¶æ€**ï¼šä½¿ç”¨ä¸Šä¸‹æ–‡å‘é‡Cåˆå§‹åŒ–è§£ç å™¨çš„éšè—çŠ¶æ€
2. **è‡ªå›å½’ç”Ÿæˆ**ï¼šæ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å…¥æ˜¯ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
3. **ç»ˆæ­¢æ¡ä»¶**ï¼šç”Ÿæˆç‰¹æ®Šæ ‡è®°`<END>`æ—¶åœæ­¢

**ä»£ç å®ç°ï¼š**

```python
class TranslationDecoder(nn.Module):
    """ç¿»è¯‘è§£ç å™¨ï¼ˆæ„å›¾ï¼šæ ¹æ®ä¸Šä¸‹æ–‡å‘é‡è‡ªå›å½’ç”Ÿæˆç›®æ ‡è¯­è¨€åºåˆ—ï¼‰"""
    
    def __init__(self, vocab_size, padding_index):
        super().__init__()
        self.embedding = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=config.EMBEDDING_DIM,
            padding_idx=padding_index
        )
        self.GRU = nn.GRU(
            input_size=config.EMBEDDING_DIM,
            hidden_size=config.HIDDEN_SIZE,
            batch_first=True
        )
        self.linear = nn.Linear(in_features=config.HIDDEN_SIZE, out_features=vocab_size)
    
    def forward(self, x, hidden_0):
        """
        å‚æ•°:
            x: [batch_size, seq_len] è¾“å…¥åºåˆ—
            hidden_0: [1, batch_size, hidden_size] åˆå§‹éšè—çŠ¶æ€
        è¿”å›:
            output: [batch_size, seq_len, vocab_size] è¯è¡¨åˆ†å¸ƒ
            hidden_n: [1, batch_size, hidden_size] æœ€ç»ˆéšè—çŠ¶æ€
        """
        embed = self.embedding(x)  # [batch, seq_len, embedding_dim]
        gru_out, hidden_n = self.GRU(embed, hidden_0)  # [batch, seq_len, hidden_size]
        output = self.linear(gru_out)  # [batch, seq_len, vocab_size]
        return output, hidden_n
```

**ä¸¤ç§è§£ç ç­–ç•¥ï¼š**

| ç­–ç•¥ | è¯´æ˜ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|------|------|
| **è´ªå¿ƒè§£ç ** | æ¯æ­¥é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯ | ç®€å•å¿«é€Ÿ | å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜ |
| **æŸæœç´¢ï¼ˆBeam Searchï¼‰** | ä¿ç•™Top-kä¸ªå€™é€‰åºåˆ— | è´¨é‡æ›´é«˜ | è®¡ç®—é‡å¤§ |

---

### 4.2.3 æ•´ä½“æ¶æ„

**ç¼–ç å™¨-è§£ç å™¨å®Œæ•´æµç¨‹ï¼š**

```mermaid
flowchart TB
    subgraph Input["è¾“å…¥åºåˆ—"]
        X1(["æˆ‘"])
        X2(["å–œæ¬¢"])
        X3(["NLP"])
    end
    
    subgraph Encoder["ç¼–ç å™¨"]
        E1["Embedding"]
        E_RNN["RNN/LSTM/GRU"]
    end
    
    subgraph Context["ä¸Šä¸‹æ–‡å‘é‡"]
        C(["C"])
    end
    
    subgraph Decoder["è§£ç å™¨"]
        D_RNN["RNN/LSTM/GRU"]
        D1["Embedding"]
        D_Out["Softmax"]
    end
    
    subgraph Output["è¾“å‡ºåºåˆ—"]
        Y1(["I"])
        Y2(["like"])
        Y3(["NLP"])
    end
    
    X1 --> E1
    X2 --> E1
    X3 --> E1
    E1 --> E_RNN --> C
    
    C --> D_RNN
    D_RNN --> D_Out --> Y1
    Y1 --> D1 --> D_RNN
    D_RNN --> D_Out --> Y2
    Y2 --> D1 --> D_RNN
    D_RNN --> D_Out --> Y3
    
    style X1 fill:#3498db,stroke:#333,stroke-width:2px,color:#fff
    style X2 fill:#3498db,stroke:#333,stroke-width:2px,color:#fff
    style X3 fill:#3498db,stroke:#333,stroke-width:2px,color:#fff
    style C fill:#e74c3c,stroke:#333,stroke-width:2px,color:#fff
    style Y1 fill:#2ecc71,stroke:#333,stroke-width:2px
    style Y2 fill:#2ecc71,stroke:#333,stroke-width:2px
    style Y3 fill:#2ecc71,stroke:#333,stroke-width:2px
```

**å›¾ä¾‹è¯´æ˜ï¼š**
- ğŸ”µ **è“è‰²**ï¼šè¾“å…¥åºåˆ—çš„è¯
- ğŸ”´ **çº¢è‰²**ï¼šä¸Šä¸‹æ–‡å‘é‡ï¼ˆä¿¡æ¯å‹ç¼©ä¸­å¿ƒï¼‰
- ğŸŸ¢ **ç»¿è‰²**ï¼šè¾“å‡ºåºåˆ—çš„è¯

**å®Œæ•´æ¨¡å‹ä»£ç ï¼š**

```python
class TranslationModel(nn.Module):
    """Seq2Seqç¿»è¯‘æ¨¡å‹ï¼ˆç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼‰"""
    
    def __init__(self, zh_vocab_size, en_vocab_size, zh_padding_index, en_padding_index):
        super().__init__()
        self.encoder = TranslationEncoder(zh_vocab_size, padding_index=zh_padding_index)
        self.decoder = TranslationDecoder(en_vocab_size, padding_index=en_padding_index)
```

---

## 4.3 æ¨¡å‹è®­ç»ƒå’Œæ¨ç†æœºåˆ¶

### 4.3.1 æ¨¡å‹è®­ç»ƒ

**è®­ç»ƒç›®æ ‡ï¼š** æœ€å¤§åŒ–ç”Ÿæˆæ­£ç¡®ç›®æ ‡åºåˆ—çš„æ¦‚ç‡

**æŸå¤±å‡½æ•°ï¼š** äº¤å‰ç†µæŸå¤±ï¼ˆCross-Entropy Lossï¼‰

$$
L = -\sum \log P(y_t | y_1, y_2, ..., y_{t-1}, x)
$$

**è®­ç»ƒæµç¨‹ï¼š**

```mermaid
flowchart TB
    subgraph Training["è®­ç»ƒæµç¨‹"]
        direction TB
        
        Input["è¾“å…¥åºåˆ— x"] --> Encoder["ç¼–ç å™¨"]
        Encoder --> Context["ä¸Šä¸‹æ–‡å‘é‡ C"]
        
        Target["ç›®æ ‡åºåˆ— y"] --> Teacher["Teacher Forcing"]
        Context --> Decoder["è§£ç å™¨"]
        Teacher --> Decoder
        
        Decoder --> Output["é¢„æµ‹è¾“å‡º Å·"]
        Output --> Loss["è®¡ç®—æŸå¤± Loss"]
        Target --> Loss
        
        Loss --> Backward["åå‘ä¼ æ’­"]
        Backward --> Update["æ›´æ–°å‚æ•°"]
    end
    
    style Input fill:#3498db,stroke:#333,stroke-width:2px,color:#fff
    style Target fill:#9b59b6,stroke:#333,stroke-width:2px,color:#fff
    style Context fill:#e74c3c,stroke:#333,stroke-width:2px,color:#fff
    style Loss fill:#f39c12,stroke:#333,stroke-width:2px
```

**Teacher Forcingï¼ˆæ•™å¸ˆå¼ºåˆ¶ï¼‰ï¼š**

åœ¨è®­ç»ƒæ—¶ï¼Œè§£ç å™¨çš„è¾“å…¥ä½¿ç”¨çœŸå®çš„ç›®æ ‡åºåˆ—ï¼ˆGround Truthï¼‰ï¼Œè€Œä¸æ˜¯ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„é¢„æµ‹è¾“å‡ºã€‚

| æ–¹å¼                  | è®­ç»ƒæ—¶è§£ç å™¨è¾“å…¥  | ä¼˜ç‚¹       | ç¼ºç‚¹                      |
| ------------------- | --------- | -------- | ----------------------- |
| **Teacher Forcing** | çœŸå®æ ‡ç­¾ yâ‚œâ‚‹â‚ | è®­ç»ƒç¨³å®šã€æ”¶æ•›å¿« | è®­ç»ƒå’Œæ¨ç†ä¸ä¸€è‡´ï¼ˆExposure Biasï¼‰ |
| **Free Running**    | æ¨¡å‹é¢„æµ‹ Å·â‚œâ‚‹â‚ | è®­ç»ƒå’Œæ¨ç†ä¸€è‡´  | è®­ç»ƒå›°éš¾ã€è¯¯å·®ç´¯ç§¯               |

**Scheduled Samplingï¼š** é€æ­¥å‡å°‘Teacher Forcingçš„æ¯”ä¾‹ï¼Œå¹³è¡¡ä¸¤ç§æ–¹å¼çš„ä¼˜ç‚¹ã€‚

**è®­ç»ƒä»£ç ï¼š**

```python
def train_one_epoch(model, dataloader, loss_fn, optimizer, device):
    """è®­ç»ƒä¸€ä¸ªè½®æ¬¡"""
    model.train()
    total_loss = 0
    
    for inputs, targets in tqdm(dataloader, desc='è®­ç»ƒ'):
        # æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
        encoder_inputs = inputs.to(device)
        targets = targets.to(device)
        
        # å‡†å¤‡è§£ç å™¨è¾“å…¥å’Œç›®æ ‡ï¼ˆæ„å›¾ï¼šTeacher Forcingç­–ç•¥ï¼‰
        decoder_inputs = targets[:, :-1]  # å»æ‰<eos>
        decoder_targets = targets[:, 1:]  # å»æ‰<sos>
        
        # ç¼–ç é˜¶æ®µ
        context_vector = model.encoder(encoder_inputs)
        
        # è§£ç é˜¶æ®µï¼ˆè‡ªå›å½’ï¼‰
        decoder_hidden = context_vector.unsqueeze(0)
        decoder_outputs = []
        seq_len = decoder_inputs.shape[1]
        
        for i in range(seq_len):
            decoder_input = decoder_inputs[:, i].unsqueeze(1)
            decoder_output, decoder_hidden = model.decoder(decoder_input, decoder_hidden)
            decoder_outputs.append(decoder_output)
        
        # åˆå¹¶è¾“å‡ºå¹¶reshape
        decoder_outputs = torch.cat(decoder_outputs, dim=1)
        decoder_outputs = decoder_outputs.reshape(-1, decoder_outputs.shape[-1])
        decoder_targets = decoder_targets.reshape(-1)
        
        # è®¡ç®—æŸå¤±å¹¶åå‘ä¼ æ’­
        loss = loss_fn(decoder_outputs, decoder_targets)
        total_loss += loss.item()
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    return total_loss / len(dataloader)
```

---

### 4.3.2 æ¨¡å‹æ¨ç†

**æ¨ç†æµç¨‹ï¼š**

```mermaid
flowchart TB
    subgraph Inference["æ¨ç†æµç¨‹"]
        direction TB
        
        Input["è¾“å…¥åºåˆ—"] --> Encoder["ç¼–ç å™¨"]
        Encoder --> Context["ä¸Šä¸‹æ–‡å‘é‡"]
        
        Context --> Decoder1["è§£ç å™¨ t=1"]
        START(["<START>"]) --> Decoder1
        Decoder1 --> Y1(["yâ‚"])
        
        Y1 --> Decoder2["è§£ç å™¨ t=2"]
        Context --> Decoder2
        Decoder2 --> Y2(["yâ‚‚"])
        
        Y2 --> Decoder3["è§£ç å™¨ t=3"]
        Context --> Decoder3
        Decoder3 --> Y3(["yâ‚ƒ"])
        
        Decoder3 --> END(["<END>"])
    end
    
    style Input fill:#3498db,stroke:#333,stroke-width:2px,color:#fff
    style Context fill:#e74c3c,stroke:#333,stroke-width:2px,color:#fff
    style START fill:#9b59b6,stroke:#333,stroke-width:2px,color:#fff
    style Y1 fill:#2ecc71,stroke:#333,stroke-width:2px
    style Y2 fill:#2ecc71,stroke:#333,stroke-width:2px
    style Y3 fill:#2ecc71,stroke:#333,stroke-width:2px
    style END fill:#f39c12,stroke:#333,stroke-width:2px
```

**æ¨ç†ä»£ç ï¼š**

```python
def predict_batch(model, inputs, en_tokenizer, device):
    """æ‰¹é‡é¢„æµ‹ï¼ˆè‡ªå›å½’ç”Ÿæˆï¼‰"""
    model.eval()
    
    with torch.no_grad():
        # ç¼–ç é˜¶æ®µ
        context_vector = model.encoder(inputs)
        
        batch_size = inputs.shape[0]
        hidden = context_vector.unsqueeze(0)
        
        # åˆå§‹åŒ–è§£ç å™¨è¾“å…¥ä¸º<sos>æ ‡è®°
        decoder_input = torch.full([batch_size, 1], en_tokenizer.sos_token_index, device=device)
        
        generated = []
        is_finished = torch.zeros(batch_size, dtype=torch.bool, device=device)
        
        # è‡ªå›å½’ç”Ÿæˆ
        for i in range(config.SEQ_LEN):
            decoder_output, hidden = model.decoder(decoder_input, hidden)
            
            # è´ªå¿ƒè§£ç 
            next_token_indexes = torch.argmax(decoder_output, dim=-1)
            generated.append(next_token_indexes)
            
            # æ›´æ–°è¾“å…¥ï¼ˆè‡ªå›å½’ç‰¹æ€§ï¼‰
            decoder_input = next_token_indexes
            
            # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆ<eos>
            is_finished |= (next_token_indexes.squeeze(1) == en_tokenizer.eos_token_index)
            if is_finished.all():
                break
        
        # å¤„ç†é¢„æµ‹ç»“æœ
        generated_tensor = torch.cat(generated, dim=1)
        generated_list = generated_tensor.tolist()
        
        # æˆªæ–­<eos>ä¹‹åçš„æ ‡è®°
        for index, sentence in enumerate(generated_list):
            if en_tokenizer.eos_token_index in sentence:
                eos_pos = sentence.index(en_tokenizer.eos_token_index)
                generated_list[index] = sentence[:eos_pos]
        
        return generated_list
```

**æ¨ç†ç­–ç•¥ï¼š**

**1. è´ªå¿ƒæœç´¢ï¼ˆGreedy Searchï¼‰**

```python
# æ¯æ­¥é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯
y_t = argmax(P(y|y_1, ..., y_{t-1}, x))
```

- **ä¼˜ç‚¹**ï¼šç®€å•å¿«é€Ÿ
- **ç¼ºç‚¹**ï¼šå±€éƒ¨æœ€ä¼˜ï¼Œå¯èƒ½é”™è¿‡å…¨å±€æœ€ä¼˜åºåˆ—

**2. æŸæœç´¢ï¼ˆBeam Searchï¼‰**

```python
# ä¿ç•™kä¸ªæœ€ä¼˜å€™é€‰åºåˆ—
# æ¯æ­¥æ‰©å±•kä¸ªå€™é€‰ï¼Œä¿ç•™å¾—åˆ†æœ€é«˜çš„kä¸ª
```

| Beam Size | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|-----------|------|---------|
| k=1 | é€€åŒ–ä¸ºè´ªå¿ƒæœç´¢ | å¿«é€Ÿæ¨ç† |
| k=3-5 | å¹³è¡¡è´¨é‡å’Œé€Ÿåº¦ | å¤§å¤šæ•°ä»»åŠ¡ |
| k=10+ | è´¨é‡æ›´é«˜ä½†æ…¢ | å¯¹è´¨é‡è¦æ±‚é«˜çš„ä»»åŠ¡ |

**3. é‡‡æ ·ç­–ç•¥ï¼ˆSamplingï¼‰**

- **Temperature Sampling**ï¼šé€šè¿‡æ¸©åº¦å‚æ•°æ§åˆ¶éšæœºæ€§
- **Top-k Sampling**ï¼šä»æ¦‚ç‡æœ€é«˜çš„kä¸ªè¯ä¸­é‡‡æ ·
- **Top-p (Nucleus) Sampling**ï¼šä»ç´¯ç§¯æ¦‚ç‡è¾¾åˆ°pçš„æœ€å°é›†åˆä¸­é‡‡æ ·

---

## 4.4 æ¡ˆä¾‹å®æ“ï¼ˆä¸­è‹±ç¿»è¯‘V1.0ï¼‰

### 4.4.1 éœ€æ±‚è¯´æ˜

**é¡¹ç›®ç›®æ ‡ï¼š** å®ç°ä¸€ä¸ªåŸºäºSeq2Seqçš„ä¸­è‹±ç¥ç»æœºå™¨ç¿»è¯‘ç³»ç»Ÿ

**åŠŸèƒ½éœ€æ±‚ï¼š**
1. è¾“å…¥ä¸­æ–‡å¥å­ï¼Œè¾“å‡ºè‹±æ–‡ç¿»è¯‘
2. æ”¯æŒå˜é•¿åºåˆ—å¤„ç†
3. ä½¿ç”¨GRUä½œä¸ºåŸºç¡€å•å…ƒ

**ç¤ºä¾‹ï¼š**

```
è¾“å…¥ï¼š"æˆ‘å–œæ¬¢è‡ªç„¶è¯­è¨€å¤„ç†"
è¾“å‡ºï¼š"I like natural language processing"
```

### 4.4.2 éœ€æ±‚åˆ†æ

**æ•°æ®æµåˆ†æï¼š**

```mermaid
flowchart LR
    subgraph DataFlow["æ•°æ®å¤„ç†æµç¨‹"]
        direction LR
        
        Raw["åŸå§‹è¯­æ–™"] --> Clean["æ•°æ®æ¸…æ´—"]
        Clean --> Tokenize["åˆ†è¯/åˆ†å­—"]
        Tokenize --> BuildVocab["æ„å»ºè¯è¡¨"]
        BuildVocab --> Encode["åºåˆ—ç¼–ç "]
        Encode --> Batch["æ‰¹å¤„ç†"]
        Batch --> Model["Seq2Seqæ¨¡å‹"]
    end
    
    style Raw fill:#3498db,stroke:#333,stroke-width:2px,color:#fff
    style Model fill:#e74c3c,stroke:#333,stroke-width:2px,color:#fff
```

**æŠ€æœ¯é€‰å‹ï¼š**

| ç»„ä»¶ | é€‰æ‹© | åŸå›  |
|------|------|------|
| ç¼–ç å™¨ | GRU | æ¯”LSTMè½»é‡ï¼Œè®­ç»ƒæ›´å¿« |
| è§£ç å™¨ | GRU | ä¸ç¼–ç å™¨ä¿æŒä¸€è‡´ |
| ä¸­æ–‡åˆ†è¯ | å­—ç¬¦çº§ | é¿å…åˆ†è¯é”™è¯¯ç´¯ç§¯ |
| è‹±æ–‡åˆ†è¯ | NLTK Treebank | æ ‡å‡†è‹±æ–‡åˆ†è¯ |
| è¯„ä¼°æŒ‡æ ‡ | BLEU-4 | æœºå™¨ç¿»è¯‘æ ‡å‡†æŒ‡æ ‡ |

### 4.4.3 é¡¹ç›®ç»“æ„

```
translation_seq2seq/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py      # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ process.py     # æ•°æ®é¢„å¤„ç†
â”‚   â”œâ”€â”€ dataset.py     # Datasetå’ŒDataLoader
â”‚   â”œâ”€â”€ model.py       # Seq2Seqæ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ train.py       # è®­ç»ƒæµç¨‹
â”‚   â”œâ”€â”€ evaluate.py    # BLEUè¯„ä¼°
â”‚   â”œâ”€â”€ predict.py     # é¢„æµ‹æ¥å£
â”‚   â””â”€â”€ tokenizer.py   # ä¸­è‹±æ–‡åˆ†è¯å™¨
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/           # åŸå§‹å¹³è¡Œè¯­æ–™(cmn.txt)
â”‚   â””â”€â”€ processed/     # å¤„ç†åçš„JSONLæ•°æ®
â”œâ”€â”€ models/            # ä¿å­˜çš„è¯è¡¨å’Œæ¨¡å‹æƒé‡
â””â”€â”€ logs/              # TensorBoardè®­ç»ƒæ—¥å¿—
```

### 4.4.4 é…ç½®æ–‡ä»¶ï¼ˆconfig.pyï¼‰

```python
"""
é…ç½®æ–‡ä»¶æ¨¡å—

ä½œè€…: Red_Moon
åˆ›å»ºæ—¥æœŸ: 2026-02
"""

from pathlib import Path

# é¡¹ç›®æ ¹ç›®å½•
ROOT_DIR = Path(__file__).parent.parent

# æ•°æ®ç›®å½•
RAW_DATA_DIR = ROOT_DIR / "data" / "raw"
PROCESSED_DATA_DIR = ROOT_DIR / "data" / "processed"
LOGS_DIR = ROOT_DIR / "logs"
MODELS_DIR = ROOT_DIR / "models"

# åºåˆ—é•¿åº¦é…ç½®
SEQ_LEN = 128

# è®­ç»ƒè¶…å‚æ•°
BATCH_SIZE = 64
EMBEDDING_DIM = 128
HIDDEN_SIZE = 256
LEARNING_RATE = 1e-3
EPOCHS = 30
```

### 4.4.5 æ•°æ®é¢„å¤„ç†ï¼ˆprocess.pyï¼‰

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from tokenizer import EnglishTokenizer, ChineseTokenizer
import config


def process():
    """æ•°æ®å¤„ç†ä¸»å‡½æ•°"""
    print("å¼€å§‹å¤„ç†æ•°æ®")
    
    # è¯»å–åŸå§‹å¹³è¡Œè¯­æ–™ï¼ˆcmn.txtæ ¼å¼ï¼šè‹±æ–‡\tä¸­æ–‡ï¼‰
    df = pd.read_csv(
        config.RAW_DATA_DIR / "cmn.txt",
        sep='\t',
        header=None,
        usecols=[0, 1],
        names=["en", "zh"],
        encoding='utf-8'
    ).dropna()
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
    
    # æ„å»ºè¯è¡¨
    ChineseTokenizer.build_vocab(train_df['zh'].tolist(), config.MODELS_DIR / 'zh_vocab.txt')
    EnglishTokenizer.build_vocab(train_df['en'].tolist(), config.MODELS_DIR / 'en_vocab.txt')
    
    # åŠ è½½è¯è¡¨
    zh_tokenizer = ChineseTokenizer.from_vocab(config.MODELS_DIR / 'zh_vocab.txt')
    en_tokenizer = EnglishTokenizer.from_vocab(config.MODELS_DIR / 'en_vocab.txt')
    
    # ç¼–ç è®­ç»ƒé›†
    train_df['zh'] = train_df['zh'].apply(lambda x: zh_tokenizer.encode(x, add_sos_eos=False))
    train_df['en'] = train_df['en'].apply(lambda x: en_tokenizer.encode(x, add_sos_eos=True))
    train_df.to_json(config.PROCESSED_DATA_DIR / 'train.jsonl', orient='records', lines=True)
    
    # ç¼–ç æµ‹è¯•é›†
    test_df['zh'] = test_df['zh'].apply(lambda x: zh_tokenizer.encode(x, add_sos_eos=False))
    test_df['en'] = test_df['en'].apply(lambda x: en_tokenizer.encode(x, add_sos_eos=True))
    test_df.to_json(config.PROCESSED_DATA_DIR / 'test.jsonl', orient='records', lines=True)
    
    print("æ•°æ®å¤„ç†å®Œæˆ")


if __name__ == '__main__':
    process()
```

### 4.4.6 åˆ†è¯å™¨ï¼ˆtokenizer.pyï¼‰

```python
"""
åˆ†è¯å™¨æ¨¡å—

ä½œè€…: Red_Moon
åˆ›å»ºæ—¥æœŸ: 2026-02
"""

import jieba
from nltk import TreebankWordTokenizer, TreebankWordDetokenizer
from tqdm import tqdm


class BaseTokenizer:
    """åˆ†è¯å™¨åŸºç±»"""
    
    pad_token = '<pad>'
    unk_token = '<unk>'
    sos_token = '<sos>'
    eos_token = '<eos>'
    
    def __init__(self, vocab_list):
        self.vocab_list = vocab_list
        self.vocab_size = len(vocab_list)
        self.word2index = {word: index for index, word in enumerate(vocab_list)}
        self.index2word = {index: word for index, word in enumerate(vocab_list)}
        
        self.pad_token_index = self.word2index[self.pad_token]
        self.unk_token_index = self.word2index[self.unk_token]
        self.sos_token_index = self.word2index[self.sos_token]
        self.eos_token_index = self.word2index[self.eos_token]
    
    @classmethod
    def tokenize(cls, text):
        raise NotImplementedError
    
    def encode(self, text, add_sos_eos=False):
        """æ–‡æœ¬ç¼–ç """
        tokens = self.tokenize(text)
        if add_sos_eos:
            tokens = [self.sos_token] + tokens + [self.eos_token]
        return [self.word2index.get(token, self.unk_token_index) for token in tokens]
    
    @classmethod
    def build_vocab(cls, sentences, vocab_path):
        """æ„å»ºè¯è¡¨"""
        vocab_set = set()
        for sentence in tqdm(sentences, desc="æ„å»ºè¯è¡¨"):
            vocab_set.update(cls.tokenize(sentence))
        
        vocab_list = [cls.pad_token, cls.unk_token, cls.sos_token, cls.eos_token]
        vocab_list += [token for token in vocab_set if token.strip() != ""]
        
        with open(vocab_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(vocab_list))
    
    @classmethod
    def from_vocab(cls, vocab_path):
        """ä»æ–‡ä»¶åŠ è½½è¯è¡¨"""
        with open(vocab_path, 'r', encoding='utf-8') as f:
            vocab_list = [line.strip() for line in f.readlines()]
        return cls(vocab_list)


class ChineseTokenizer(BaseTokenizer):
    """ä¸­æ–‡åˆ†è¯å™¨ï¼ˆå­—ç¬¦çº§ï¼‰"""
    
    @classmethod
    def tokenize(cls, text):
        return list(text)


class EnglishTokenizer(BaseTokenizer):
    """è‹±æ–‡åˆ†è¯å™¨"""
    
    tokenizer = TreebankWordTokenizer()
    detokenizer = TreebankWordDetokenizer()
    
    @classmethod
    def tokenize(cls, text):
        return cls.tokenizer.tokenize(text)
    
    def decode(self, indexes):
        """ç´¢å¼•è§£ç ä¸ºæ–‡æœ¬"""
        tokens = [self.index2word[index] for index in indexes]
        return self.detokenizer.detokenize(tokens)
```

### 4.4.7 è¿è¡Œç¤ºä¾‹

```bash
# 1. æ•°æ®é¢„å¤„ç†
python src/process.py

# 2. è®­ç»ƒæ¨¡å‹
python src/train.py

# 3. è¯„ä¼°æ¨¡å‹ï¼ˆBLEU-4ï¼‰
python src/evaluate.py

# 4. äº¤äº’å¼ç¿»è¯‘
python src/predict.py
```

**é¢„æµ‹æ•ˆæœç¤ºä¾‹ï¼š**

```
========================================
æ¬¢è¿ä½¿ç”¨ç¿»è¯‘æ¨¡å‹(è¾“å…¥qæˆ–è€…quité€€å‡º)
========================================
ä¸­æ–‡ï¼š ä½ å¥½ä¸–ç•Œ
ç¿»è¯‘ç»“æœ: Hello world
----------------------------------------
ä¸­æ–‡ï¼š æˆ‘å–œæ¬¢è‡ªç„¶è¯­è¨€å¤„ç†
ç¿»è¯‘ç»“æœ: I like natural language processing
----------------------------------------
```

---

## 4.5 å­˜åœ¨é—®é¢˜

### 4.5.1 ä¿¡æ¯ç“¶é¢ˆé—®é¢˜

**é—®é¢˜æè¿°ï¼š**

ç¼–ç å™¨å°†æ‰€æœ‰è¾“å…¥ä¿¡æ¯å‹ç¼©åˆ°ä¸€ä¸ªå›ºå®šç»´åº¦çš„ä¸Šä¸‹æ–‡å‘é‡ä¸­ï¼Œå¯¼è‡´ï¼š
- é•¿åºåˆ—ä¿¡æ¯ä¸¢å¤±
- éš¾ä»¥è®°ä½åºåˆ—å¼€å¤´çš„ä¿¡æ¯

**æ•°å­¦è§£é‡Šï¼š**

```
è¾“å…¥åºåˆ—é•¿åº¦: n
ä¸Šä¸‹æ–‡å‘é‡ç»´åº¦: d
ä¿¡æ¯å‹ç¼©æ¯”: n/d ï¼ˆåºåˆ—è¶Šé•¿ï¼Œå‹ç¼©æ¯”è¶Šå¤§ï¼‰
```

**å¯è§†åŒ–ï¼š**

```mermaid
flowchart TB
    subgraph Bottleneck["ä¿¡æ¯ç“¶é¢ˆç¤ºæ„"]
        direction LR
        
        LongInput["é•¿è¾“å…¥åºåˆ—<br/>[xâ‚, xâ‚‚, ..., xâ‚â‚€â‚€]"] --> Encoder["ç¼–ç å™¨"]
        Encoder --> Context["å›ºå®šç»´åº¦ä¸Šä¸‹æ–‡å‘é‡ C<br/>(å¦‚512ç»´)"]
        Context --> Decoder["è§£ç å™¨"]
        Decoder --> Output["è¾“å‡ºåºåˆ—<br/>[yâ‚, yâ‚‚, ..., yâ‚…â‚€]"]
        
        Loss["ä¿¡æ¯ä¸¢å¤±åŒºåŸŸ"] -.-> Context
    end
    
    style LongInput fill:#3498db,stroke:#333,stroke-width:2px,color:#fff
    style Context fill:#e74c3c,stroke:#333,stroke-width:2px,color:#fff
    style Loss fill:#f39c12,stroke:#333,stroke-width:2px
```

**è§£å†³æ–¹æ¡ˆï¼š**
- **æ³¨æ„åŠ›æœºåˆ¶**ï¼šè§£ç å™¨å¯ä»¥å…³æ³¨è¾“å…¥åºåˆ—çš„ä¸åŒéƒ¨åˆ†
- **Transformer**ï¼šå®Œå…¨åŸºäºæ³¨æ„åŠ›ï¼Œæ— ä¿¡æ¯ç“¶é¢ˆ

### 4.5.2 é•¿åºåˆ—å»ºæ¨¡å›°éš¾

**é—®é¢˜æè¿°ï¼š**

RNN/LSTM/GRUç­‰å¾ªç¯ç»“æ„åœ¨å¤„ç†é•¿åºåˆ—æ—¶ï¼š
- æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
- éš¾ä»¥æ•è·è¿œè·ç¦»ä¾èµ–

**ç¤ºä¾‹ï¼š**

```
è¾“å…¥ï¼š"è™½ç„¶ä»–å¾ˆå¿™ï¼Œä½†æ˜¯...ï¼ˆä¸­é—´å¾ˆé•¿ï¼‰...ä»–è¿˜æ˜¯æ¥äº†"
é—®é¢˜ï¼šæ¨¡å‹éš¾ä»¥å°†"è™½ç„¶"å’Œ"ä½†æ˜¯"å…³è”èµ·æ¥
```

### 4.5.3 è®­ç»ƒå’Œæ¨ç†ä¸ä¸€è‡´

**é—®é¢˜æè¿°ï¼š**

Teacher Forcingå¯¼è‡´è®­ç»ƒæ—¶ä½¿ç”¨çœŸå®æ ‡ç­¾ï¼Œè€Œæ¨ç†æ—¶ä½¿ç”¨æ¨¡å‹é¢„æµ‹ï¼Œé€ æˆExposure Biasã€‚

**å½±å“ï¼š**
- è®­ç»ƒæ—¶è¡¨ç°å¥½ï¼Œæ¨ç†æ—¶è¡¨ç°å·®
- è¯¯å·®ç´¯ç§¯ï¼ˆç”Ÿæˆä¸€ä¸ªè¯é”™è¯¯ï¼Œåç»­å¯èƒ½å…¨é”™ï¼‰

**ç¼“è§£æ–¹æ³•ï¼š**
- Scheduled Sampling
- å¼ºåŒ–å­¦ä¹ è®­ç»ƒ
- å¯¹æŠ—è®­ç»ƒ

### 4.5.4 ç¼ºä¹å¯è§£é‡Šæ€§

**é—®é¢˜æè¿°ï¼š**

ä¸Šä¸‹æ–‡å‘é‡æ˜¯é»‘ç›’è¡¨ç¤ºï¼Œéš¾ä»¥ç†è§£æ¨¡å‹"å…³æ³¨"äº†è¾“å…¥çš„å“ªäº›éƒ¨åˆ†ã€‚

**å¯¹æ¯”ï¼ˆå¼•å…¥æ³¨æ„åŠ›åï¼‰ï¼š**

| ç‰¹æ€§ | åŸºç¡€Seq2Seq | å¸¦æ³¨æ„åŠ›çš„Seq2Seq |
|------|------------|------------------|
| å¯è§£é‡Šæ€§ | å·® | å¥½ï¼ˆæ³¨æ„åŠ›æƒé‡å¯è§†åŒ–ï¼‰ |
| é•¿åºåˆ—æ€§èƒ½ | å·® | å¥½ |
| è®¡ç®—å¤æ‚åº¦ | O(n) | O(nÂ²) |

---

## 4.6 æ‹“å±•ï¼šæ³¨æ„åŠ›æœºåˆ¶ç®€ä»‹

### 4.6.1 ä¸ºä»€ä¹ˆéœ€è¦æ³¨æ„åŠ›

**æ ¸å¿ƒæ€æƒ³ï¼š** è§£ç å™¨åœ¨ç”Ÿæˆæ¯ä¸ªè¯æ—¶ï¼ŒåŠ¨æ€åœ°å…³æ³¨è¾“å…¥åºåˆ—çš„ä¸åŒéƒ¨åˆ†ã€‚

```mermaid
flowchart TB
    subgraph Attention["æ³¨æ„åŠ›æœºåˆ¶ç¤ºæ„"]
        direction TB
        
        Input["è¾“å…¥ï¼šæˆ‘ å–œæ¬¢ è‡ªç„¶ è¯­è¨€ å¤„ç†"] --> Encoder
        
        subgraph Encoding["ç¼–ç å™¨è¾“å‡º"]
            H1(["hâ‚"])
            H2(["hâ‚‚"])
            H3(["hâ‚ƒ"])
            H4(["hâ‚„"])
            H5(["hâ‚…"])
        end
        
        Decoder1["è§£ç å™¨ç”Ÿæˆ 'I'"] --> Weight1["å…³æ³¨ hâ‚"]
        Decoder2["è§£ç å™¨ç”Ÿæˆ 'like'"] --> Weight2["å…³æ³¨ hâ‚‚"]
        Decoder3["è§£ç å™¨ç”Ÿæˆ 'NLP'"] --> Weight3["å…³æ³¨ hâ‚ƒ-hâ‚…"]
        
        H1 --> Weight1
        H2 --> Weight2
        H3 --> Weight3
        H4 --> Weight3
        H5 --> Weight3
    end
    
    style Input fill:#3498db,stroke:#333,stroke-width:2px,color:#fff
    style Decoder1 fill:#e74c3c,stroke:#333,stroke-width:2px,color:#fff
    style Decoder2 fill:#e74c3c,stroke:#333,stroke-width:2px,color:#fff
    style Decoder3 fill:#e74c3c,stroke:#333,stroke-width:2px,color:#fff
```

### 4.6.2 æ³¨æ„åŠ›æœºåˆ¶åŸç†

**è®¡ç®—æ­¥éª¤ï¼š**

1. **è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°**ï¼šè¡¡é‡è§£ç å™¨å½“å‰çŠ¶æ€ä¸æ¯ä¸ªç¼–ç å™¨çŠ¶æ€çš„ç›¸å…³æ€§
2. **è®¡ç®—æ³¨æ„åŠ›æƒé‡**ï¼šä½¿ç”¨Softmaxå½’ä¸€åŒ–åˆ†æ•°
3. **è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡**ï¼šåŠ æƒæ±‚å’Œç¼–ç å™¨çŠ¶æ€

**æ•°å­¦è¡¨è¾¾ï¼š**

```
æ³¨æ„åŠ›åˆ†æ•°:  score(sâ‚œ, háµ¢) = sâ‚œáµ€ Â· háµ¢
æ³¨æ„åŠ›æƒé‡:  Î±â‚œáµ¢ = softmax(score(sâ‚œ, háµ¢))
ä¸Šä¸‹æ–‡å‘é‡:  câ‚œ = Î£ Î±â‚œáµ¢ Â· háµ¢
```

### 4.6.3 æ³¨æ„åŠ›ç±»å‹

| æ³¨æ„åŠ›ç±»å‹ | è®¡ç®—æ–¹å¼ | ç‰¹ç‚¹ |
|-----------|---------|------|
| **Dot Product** | score = sáµ€ Â· h | ç®€å•é«˜æ•ˆï¼Œè¦æ±‚ç»´åº¦ç›¸åŒ |
| **Scaled Dot Product** | score = (sáµ€ Â· h) / âˆšdâ‚– | Transformerä¸­ä½¿ç”¨ï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤± |
| **Additive (Bahdanau)** | score = váµ€ Â· tanh(Wâ‚›Â·s + Wâ‚•Â·h) | çµæ´»ï¼Œå¯å­¦ä¹ å‚æ•° |
| **Multi-Head** | å¤šç»„å¹¶è¡Œæ³¨æ„åŠ› | Transformeræ ¸å¿ƒï¼Œæ•è·ä¸åŒå­ç©ºé—´ä¿¡æ¯ |

---

## Seq2Seq vs ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”

| ç‰¹æ€§ | ä¼ ç»Ÿç»Ÿè®¡æœºå™¨ç¿»è¯‘(SMT) | ç¥ç»æœºå™¨ç¿»è¯‘(NMT/Seq2Seq) |
|------|---------------------|--------------------------|
| ç‰¹å¾å·¥ç¨‹ | éœ€è¦å¤æ‚çš„ç‰¹å¾å·¥ç¨‹ | ç«¯åˆ°ç«¯å­¦ä¹ ï¼Œæ— éœ€ç‰¹å¾å·¥ç¨‹ |
| ç¿»è¯‘æµç•…åº¦ | ç‰‡æ®µåŒ–ï¼Œä¸æµç•… | æ›´æµç•…è‡ªç„¶ |
| è®­ç»ƒæ•°æ® | éœ€è¦å¤§é‡å¯¹é½è¯­æ–™ | éœ€è¦å¤§é‡å¹³è¡Œè¯­æ–™ |
| è®­ç»ƒé€Ÿåº¦ | å¿« | æ…¢ï¼ˆéœ€è¦GPUï¼‰ |
| å¯è§£é‡Šæ€§ | è¾ƒå¥½ï¼ˆæœ‰ç¿»è¯‘è§„åˆ™ï¼‰ | è¾ƒå·®ï¼ˆé»‘ç›’æ¨¡å‹ï¼‰ |
| å¤„ç†OOV | è¾ƒå¥½ | è¾ƒå·®ï¼ˆéœ€è¦BPEç­‰å­è¯æ–¹æ³•ï¼‰ |

---

## ç›¸å…³æ–‡æ¡£

- [RNNï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œï¼‰](./03_RNN.md) - Seq2Seqçš„åŸºç¡€å•å…ƒ
- [LSTMï¼ˆé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼‰](./03_LSTM.md) - å¸¸ç”¨ç¼–ç å™¨/è§£ç å™¨
- [GRUï¼ˆé—¨æ§å¾ªç¯å•å…ƒï¼‰](./03_GRU.md) - LSTMçš„è½»é‡æ›¿ä»£
- [é™„å½•ï¼šTensorBoardä½¿ç”¨æŒ‡å—](./é™„å½•_TensorBoardä½¿ç”¨æŒ‡å—.md) - è®­ç»ƒå¯è§†åŒ–
- [é™„å½•ï¼šBLEUä½¿ç”¨æŒ‡å—](./é™„å½•_BLEUä½¿ç”¨æŒ‡å—.md) - ç¿»è¯‘è´¨é‡è¯„ä¼°

---

## å‚è€ƒèµ„æº

- PyTorchå®˜æ–¹æ•™ç¨‹ï¼šhttps://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html
- ç»å…¸è®ºæ–‡ï¼š
  - "Sequence to Sequence Learning with Neural Networks" (2014) - Sutskever et al.
  - "Neural Machine Translation by Jointly Learning to Align and Translate" (2015) - Bahdanau et al.
