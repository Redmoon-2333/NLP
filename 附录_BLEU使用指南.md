# é™„å½•ï¼šBLEUè¯„ä¼°æŒ‡æ ‡ä½¿ç”¨æŒ‡å—

## ç¬¬1ç«  æ¦‚è¿°

### 1.1 ä»€ä¹ˆæ˜¯BLEUï¼Ÿ

**BLEU**ï¼ˆBilingual Evaluation Understudyï¼ŒåŒè¯­è¯„ä¼°æ›¿è¡¥ï¼‰æ˜¯2002å¹´ç”±IBMæå‡ºçš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡æœºå™¨ç”Ÿæˆæ–‡æœ¬ä¸å‚è€ƒæ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚å®ƒé€šè¿‡è®¡ç®—n-gramçš„ç²¾ç¡®ç‡æ¥è¯„ä¼°ç”Ÿæˆè´¨é‡ï¼Œæ˜¯æœºå™¨ç¿»è¯‘é¢†åŸŸæœ€å¹¿æ³›ä½¿ç”¨çš„è¯„ä¼°æ ‡å‡†ä¹‹ä¸€ã€‚

**æ ¸å¿ƒåŸç†ï¼š**

BLEUåŸºäº**n-gramç²¾ç¡®ç‡**çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œä¸»è¦è§£å†³ä»¥ä¸‹é—®é¢˜ï¼š
- ç®€å•ç²¾ç¡®ç‡å®¹æ˜“è¢«é‡å¤è¯ç¨€é‡Š
- éœ€è¦æƒ©ç½šè¿‡çŸ­çš„ç”Ÿæˆåºåˆ—
- éœ€è¦æ”¯æŒå¤šä¸ªå‚è€ƒè¯‘æ–‡

**æ•°å­¦å…¬å¼ï¼š**

$$
\text{BLEU} = \text{BP} \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)
$$

å…¶ä¸­ï¼š
- $p_n$ï¼šn-gramç²¾ç¡®ç‡
- $w_n$ï¼šn-gramæƒé‡ï¼ˆé€šå¸¸å‡åŒ€åˆ†å¸ƒï¼‰
- $\text{BP}$ï¼šç®€çŸ­æƒ©ç½šå› å­ï¼ˆBrevity Penaltyï¼‰

$$
\text{BP} = \begin{cases} 
1 & \text{if } c > r \\
e^{1-r/c} & \text{if } c \leq r
\end{cases}
$$

$c$ä¸ºå€™é€‰è¯‘æ–‡é•¿åº¦ï¼Œ$r$ä¸ºæœ€æ¥è¿‘å€™é€‰é•¿åº¦çš„å‚è€ƒè¯‘æ–‡é•¿åº¦ã€‚

**æ ¸å¿ƒåŠŸèƒ½ï¼š**
- ğŸ“Š **è‡ªåŠ¨é‡åŒ–è¯„ä¼°**ï¼šå°†ç”Ÿæˆè´¨é‡è½¬åŒ–ä¸º0-1ä¹‹é—´çš„æ•°å€¼
- ğŸ” **å¤šç²’åº¦åˆ†æ**ï¼šæ”¯æŒ1-gramåˆ°4-gramçš„å¤šå±‚æ¬¡è¯„ä¼°
- ğŸ“ˆ **å¯æ¯”è¾ƒæ€§**ï¼šæ ‡å‡†åŒ–æŒ‡æ ‡ä¾¿äºä¸åŒæ¨¡å‹é—´å¯¹æ¯”
- ğŸ¯ **å¤šå‚è€ƒæ”¯æŒ**ï¼šå¯åŒæ—¶å¯¹æ¯”å¤šä¸ªå‚è€ƒè¯‘æ–‡

**é€‚ç”¨åœºæ™¯ï¼š**
- æœºå™¨ç¿»è¯‘ç³»ç»Ÿè¯„ä¼°
- æ–‡æœ¬æ‘˜è¦è´¨é‡è¯„ä¼°
- å›¾åƒæè¿°ç”Ÿæˆè¯„ä¼°
- å¯¹è¯ç³»ç»Ÿå›å¤è¯„ä¼°

### 1.2 ä¸ºä»€ä¹ˆéœ€è¦BLEUï¼Ÿ

**äººå·¥è¯„ä¼°çš„æŒ‘æˆ˜ï¼š**

| é—®é¢˜ | è¯´æ˜ | BLEUçš„è§£å†³æ–¹æ¡ˆ |
|------|------|---------------|
| **æˆæœ¬é«˜æ˜‚** | äººå·¥è¯„ä¼°éœ€è¦ä¸“ä¸šäººå‘˜å’Œæ—¶é—´ | è‡ªåŠ¨åŒ–è®¡ç®—ï¼Œå³æ—¶å‡ºç»“æœ |
| **ä¸»è§‚æ€§å¼º** | ä¸åŒè¯„ä¼°è€…æ ‡å‡†ä¸ä¸€è‡´ | åŸºäºç»Ÿè®¡çš„å®¢è§‚æŒ‡æ ‡ |
| **éš¾ä»¥å¤ç°** | äººå·¥è¯„ä¼°ç»“æœéš¾ä»¥é‡å¤éªŒè¯ | ç›¸åŒè¾“å…¥å¿…å¾—ç›¸åŒè¾“å‡º |
| **è§„æ¨¡å—é™** | æ— æ³•è¯„ä¼°å¤§è§„æ¨¡æ•°æ® | å¯æ‰¹é‡å¤„ç†ä»»æ„è§„æ¨¡æ•°æ® |

**ä½¿ç”¨å‰åå¯¹æ¯”ï¼š**

```
âŒ ä¸ä½¿ç”¨BLEUï¼š
äººå·¥é˜…è¯»1000æ¡ç¿»è¯‘ç»“æœï¼Œè€—æ—¶æ•°å¤©
è¯„ä¼°æ ‡å‡†å› äººè€Œå¼‚ï¼Œç»“æœéš¾ä»¥å¯¹æ¯”

âœ… ä½¿ç”¨BLEUï¼š
å‡ ç§’é’Ÿè®¡ç®—å®Œæˆ
æ ‡å‡†åŒ–åˆ†æ•°ï¼Œä¾¿äºæ¨ªå‘å¯¹æ¯”
å¿«é€Ÿè¿­ä»£æ¨¡å‹ï¼ŒåŠæ—¶å‘ç°é—®é¢˜
```

**å…¸å‹åº”ç”¨åœºæ™¯ï¼š**
1. **æ¨¡å‹å¼€å‘**ï¼šå¿«é€ŸéªŒè¯ç¿»è¯‘æ¨¡å‹æ”¹è¿›æ•ˆæœ
2. **è®ºæ–‡æŠ•ç¨¿**ï¼šæä¾›æ ‡å‡†åŒ–çš„å®éªŒå¯¹æ¯”æ•°æ®
3. **ç”Ÿäº§éƒ¨ç½²**ï¼šç›‘æ§çº¿ä¸Šç¿»è¯‘è´¨é‡æ³¢åŠ¨
4. **ç«èµ›è¯„ä¼°**ï¼šæœºå™¨ç¿»è¯‘æ¯”èµ›çš„å®˜æ–¹è¯„ä¼°æŒ‡æ ‡

**âš ï¸ æ³¨æ„äº‹é¡¹ï¼š**
- BLEUä¸äººå·¥è¯„ä¼°ç›¸å…³æ€§çº¦0.7-0.8ï¼Œå¹¶éå®Œç¾æ›¿ä»£
- å¯¹äºåˆ›æ„æ€§æ–‡æœ¬ï¼ˆè¯—æ­Œã€å¹¿å‘Šï¼‰è¯„ä¼°æ•ˆæœæœ‰é™
- åº”ç»“åˆå…¶ä»–æŒ‡æ ‡ï¼ˆROUGEã€METEORï¼‰ç»¼åˆè¯„ä¼°

---

## ç¬¬2ç«  å®‰è£…ä¸å‡†å¤‡

### 2.1 å®‰è£…æ–¹æ³•

**æ–¹æ³•ä¸€ï¼šé€šè¿‡pipå®‰è£…sacrebleuï¼ˆæ¨èï¼‰**

```bash
# å®‰è£…sacrebleuï¼ˆæ ‡å‡†åŒ–BLEUå®ç°ï¼‰
pip install sacrebleu

# éªŒè¯å®‰è£…
python -c "import sacrebleu; print(sacrebleu.__version__)"
```

**æ–¹æ³•äºŒï¼šä½¿ç”¨nltkåº“**

```bash
# å®‰è£…nltk
pip install nltk

# ä¸‹è½½BLEUæ‰€éœ€æ•°æ®
python -c "import nltk; nltk.download('punkt')"
```

**æ–¹æ³•ä¸‰ï¼šä½¿ç”¨torchtextï¼ˆPyTorchç”¨æˆ·ï¼‰**

```bash
# å®‰è£…torchtext
pip install torchtext

# BLEUæŒ‡æ ‡å·²åŒ…å«åœ¨å†…
```

### 2.2 éªŒè¯å®‰è£…

**æµ‹è¯•BLEUæ˜¯å¦æ­£ç¡®å®‰è£…ï¼š**

```python
# test_bleu.py
import sacrebleu

# å€™é€‰è¯‘æ–‡
hypothesis = ["the cat is on the mat"]

# å‚è€ƒè¯‘æ–‡ï¼ˆæ”¯æŒå¤šä¸ªï¼‰
references = [["the cat is on the mat"]]

# è®¡ç®—BLEU
bleu = sacrebleu.corpus_bleu(hypothesis, references)
print(f"BLEUåˆ†æ•°: {bleu.score}")
print(f"å®Œç¾åŒ¹é…çš„BLEUåº”ä¸º100: {bleu.score == 100}")
```

**è¿è¡Œæµ‹è¯•ï¼š**

```bash
python test_bleu.py
```

å¦‚æœè¾“å‡º `BLEUåˆ†æ•°: 100.0`ï¼Œè¯´æ˜å®‰è£…æˆåŠŸï¼

---

## ç¬¬3ç«  åŸºç¡€ä½¿ç”¨

### 3.1 æ ¸å¿ƒæ¦‚å¿µ

ä½¿ç”¨BLEUçš„åŸºæœ¬æµç¨‹ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. å‡†å¤‡æ–‡æœ¬     â”‚  â†’   â”‚  2. è®¡ç®—BLEU    â”‚  â†’   â”‚  3. åˆ†æç»“æœ    â”‚
â”‚  (å€™é€‰+å‚è€ƒ)    â”‚      â”‚  (è°ƒç”¨API)      â”‚      â”‚  (è§£è¯»åˆ†æ•°)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  åˆ†è¯å¤„ç†              sacrebleu.corpus_bleu    å¯¹æ¯”åŸºçº¿/å†å²æ•°æ®
```

### 3.2 å¿«é€Ÿå…¥é—¨ç¤ºä¾‹

**å•å¥è¯„ä¼°ï¼š**

```python
from sacrebleu import sentence_bleu

# å€™é€‰è¯‘æ–‡ï¼ˆæœºå™¨ç¿»è¯‘è¾“å‡ºï¼‰
candidate = "the cat is on the mat"

# å‚è€ƒè¯‘æ–‡ï¼ˆäººå·¥ç¿»è¯‘ï¼‰
reference = "the cat is on the mat"

# è®¡ç®—å•å¥BLEU
bleu = sentence_bleu(candidate, [reference])
print(f"BLEU: {bleu.score}")
```

**è¯­æ–™çº§è¯„ä¼°ï¼ˆæ¨èï¼‰ï¼š**

```python
import sacrebleu

# å€™é€‰è¯‘æ–‡åˆ—è¡¨
hypotheses = [
    "the cat is on the mat",
    "there is a cat on the mat"
]

# å‚è€ƒè¯‘æ–‡åˆ—è¡¨ï¼ˆæ¯ä¸ªå€™é€‰å¯¹åº”ä¸€ä¸ªå‚è€ƒåˆ—è¡¨ï¼‰
references = [
    ["the cat is on the mat"],
    ["there is a cat on the mat"]
]

# è®¡ç®—è¯­æ–™çº§BLEU
bleu = sacrebleu.corpus_bleu(hypotheses, references)
print(f"BLEU: {bleu.score:.2f}")
print(f"1-gram: {bleu.precisions[0]:.2f}")
print(f"2-gram: {bleu.precisions[1]:.2f}")
print(f"3-gram: {bleu.precisions[2]:.2f}")
print(f"4-gram: {bleu.precisions[3]:.2f}")
print(f"BP: {bleu.bp:.4f}")
print(f"ç³»ç»Ÿé•¿åº¦: {bleu.sys_len}, å‚è€ƒé•¿åº¦: {bleu.ref_len}")
```

### 3.3 å¤šå‚è€ƒè¯‘æ–‡æ”¯æŒ

**å®é™…åœºæ™¯ä¸­é€šå¸¸æœ‰å¤šä¸ªå‚è€ƒè¯‘æ–‡ï¼š**

```python
import sacrebleu

hypotheses = ["the cat is on the mat"]

# å¤šä¸ªå‚è€ƒè¯‘æ–‡
references = [[
    "the cat is on the mat",
    "there is a cat on the mat",
    "a cat is sitting on the mat"
]]

bleu = sacrebleu.corpus_bleu(hypotheses, references)
print(f"å¤šå‚è€ƒBLEU: {bleu.score:.2f}")
```

### 3.4 ä¸åŒn-gramé…ç½®

**é»˜è®¤ä½¿ç”¨4-gramï¼Œå¯è‡ªå®šä¹‰ï¼š**

```python
import sacrebleu

hypotheses = ["the cat is on the mat"]
references = [["the cat is on the mat"]]

# ä»…ä½¿ç”¨1-gramï¼ˆBLEU-1ï¼‰
bleu_1 = sacrebleu.corpus_bleu(
    hypotheses, references,
    max_ngram_order=1
)

# ä»…ä½¿ç”¨2-gramï¼ˆBLEU-2ï¼‰
bleu_2 = sacrebleu.corpus_bleu(
    hypotheses, references,
    max_ngram_order=2
)

print(f"BLEU-1: {bleu_1.score:.2f}")
print(f"BLEU-2: {bleu_2.score:.2f}")
```

### 3.5 å¹³æ»‘ç­–ç•¥

**å½“n-gramåŒ¹é…æ•°ä¸º0æ—¶ï¼Œéœ€è¦å¹³æ»‘å¤„ç†ï¼š**

```python
import sacrebleu

# æç«¯æƒ…å†µï¼šå‡ ä¹æ²¡æœ‰åŒ¹é…çš„n-gram
hypotheses = ["a b c d e f g"]
references = [["x y z w v u t"]]

# ä¸åŒå¹³æ»‘ç­–ç•¥
bleu_exp = sacrebleu.corpus_bleu(
    hypotheses, references,
    smooth_method='exp'  # æŒ‡æ•°å¹³æ»‘ï¼ˆé»˜è®¤ï¼‰
)

bleu_floor = sacrebleu.corpus_bleu(
    hypotheses, references,
    smooth_method='floor'  # åŠ æå°å€¼
)

print(f"æŒ‡æ•°å¹³æ»‘BLEU: {bleu_exp.score:.4f}")
print(f"Floorå¹³æ»‘BLEU: {bleu_floor.score:.4f}")
```

**å¹³æ»‘æ–¹æ³•å¯¹æ¯”ï¼š**

| æ–¹æ³• | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|
| `exp` | æŒ‡æ•°è¡°å‡å¹³æ»‘ | ä¸€èˆ¬æƒ…å†µï¼ˆé»˜è®¤ï¼‰ |
| `floor` | åŠ æå°å€¼ | çŸ­å¥è¯„ä¼° |
| `add-k` | åŠ kå¹³æ»‘ | éœ€è¦è°ƒæ•´å¹³æ»‘å¼ºåº¦ |
| `none` | ä¸å¹³æ»‘ | åŒ¹é…è¾ƒå¤šæ—¶ |

---

## ç¬¬4ç«  å®æˆ˜åº”ç”¨

### 4.1 æœºå™¨ç¿»è¯‘è¯„ä¼°å®Œæ•´æµç¨‹

```python
import sacrebleu
from typing import List, Tuple

def evaluate_translation(
    predictions: List[str],
    references: List[List[str]],
    src_sentences: List[str] = None
) -> dict:
    """
    è¯„ä¼°æœºå™¨ç¿»è¯‘è´¨é‡
    
    Args:
        predictions: æ¨¡å‹é¢„æµ‹çš„è¯‘æ–‡åˆ—è¡¨
        references: å‚è€ƒè¯‘æ–‡åˆ—è¡¨ï¼ˆæ¯ä¸ªé¢„æµ‹å¯¹åº”ä¸€ä¸ªå‚è€ƒåˆ—è¡¨ï¼‰
        src_sentences: æºè¯­è¨€å¥å­ï¼ˆå¯é€‰ï¼Œç”¨äºé”™è¯¯åˆ†æï¼‰
    
    Returns:
        åŒ…å«å„é¡¹æŒ‡æ ‡çš„å­—å…¸
    """
    # è®¡ç®—BLEU
    bleu = sacrebleu.corpus_bleu(predictions, references)
    
    # è®¡ç®—æ¯ä¸ªå¥å­çš„BLEUï¼ˆç”¨äºåˆ†æï¼‰
    sentence_bleus = [
        sacrebleu.sentence_bleu(pred, refs).score
        for pred, refs in zip(predictions, references)
    ]
    
    results = {
        'bleu': bleu.score,
        'bleu_1': bleu.precisions[0],
        'bleu_2': bleu.precisions[1],
        'bleu_3': bleu.precisions[2],
        'bleu_4': bleu.precisions[3],
        'brevity_penalty': bleu.bp,
        'system_length': bleu.sys_len,
        'reference_length': bleu.ref_len,
        'mean_sentence_bleu': sum(sentence_bleus) / len(sentence_bleus),
        'min_sentence_bleu': min(sentence_bleus),
        'max_sentence_bleu': max(sentence_bleus)
    }
    
    # æ‰¾å‡ºä½è´¨é‡ç¿»è¯‘
    if src_sentences:
        low_quality = [
            (src, pred, refs, sb) 
            for src, pred, refs, sb in zip(
                src_sentences, predictions, references, sentence_bleus
            )
            if sb < 10.0  # BLEUä½äº10è§†ä¸ºä½è´¨é‡
        ]
        results['low_quality_count'] = len(low_quality)
        results['low_quality_examples'] = low_quality[:5]  # ä¿ç•™å‰5ä¸ªç¤ºä¾‹
    
    return results


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # æ¨¡æ‹Ÿç¿»è¯‘ç»“æœ
    predictions = [
        "hello world",
        "machine translation is useful",
        "deep learning improves quality"
    ]
    
    references = [
        ["hello world", "hi world"],
        ["machine translation is helpful"],
        ["deep learning improves the quality"]
    ]
    
    src_sentences = [
        "ä½ å¥½ ä¸–ç•Œ",
        "æœºå™¨ç¿»è¯‘å¾ˆæœ‰ç”¨",
        "æ·±åº¦å­¦ä¹ æé«˜äº†è´¨é‡"
    ]
    
    results = evaluate_translation(predictions, references, src_sentences)
    
    print("=" * 50)
    print("ç¿»è¯‘è´¨é‡è¯„ä¼°æŠ¥å‘Š")
    print("=" * 50)
    print(f"æ•´ä½“BLEUåˆ†æ•°: {results['bleu']:.2f}")
    print(f"BLEU-1: {results['bleu_1']:.2f}")
    print(f"BLEU-2: {results['bleu_2']:.2f}")
    print(f"BLEU-3: {results['bleu_3']:.2f}")
    print(f"BLEU-4: {results['bleu_4']:.2f}")
    print(f"ç®€çŸ­æƒ©ç½š: {results['brevity_penalty']:.4f}")
    print(f"ä½è´¨é‡ç¿»è¯‘æ•°é‡: {results.get('low_quality_count', 0)}")
```

### 4.2 è®­ç»ƒè¿‡ç¨‹ä¸­çš„BLEUç›‘æ§

```python
import sacrebleu
from torch.utils.tensorboard import SummaryWriter
import torch

class BLEUTracker:
    """è®­ç»ƒè¿‡ç¨‹ä¸­è¿½è¸ªBLEUåˆ†æ•°"""
    
    def __init__(self, log_dir: str = 'runs/translation'):
        self.writer = SummaryWriter(log_dir)
        self.best_bleu = 0.0
        self.history = []
    
    def compute_bleu(
        self,
        predictions: List[str],
        references: List[List[str]],
        step: int,
        prefix: str = 'val'
    ) -> float:
        """è®¡ç®—å¹¶è®°å½•BLEU"""
        bleu = sacrebleu.corpus_bleu(predictions, references)
        
        # è®°å½•åˆ°TensorBoard
        self.writer.add_scalar(f'{prefix}/BLEU', bleu.score, step)
        self.writer.add_scalar(f'{prefix}/BLEU-1', bleu.precisions[0], step)
        self.writer.add_scalar(f'{prefix}/BLEU-4', bleu.precisions[3], step)
        self.writer.add_scalar(f'{prefix}/BP', bleu.bp, step)
        
        # æ›´æ–°æœ€ä½³åˆ†æ•°
        if bleu.score > self.best_bleu:
            self.best_bleu = bleu.score
            self.writer.add_scalar(f'{prefix}/Best_BLEU', bleu.score, step)
        
        self.history.append({
            'step': step,
            'bleu': bleu.score,
            'bleu_1': bleu.precisions[0],
            'bleu_4': bleu.precisions[3]
        })
        
        return bleu.score
    
    def close(self):
        self.writer.close()


# è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨
def train_epoch(model, dataloader, tracker, epoch):
    # ... è®­ç»ƒä»£ç  ...
    
    # éªŒè¯é˜¶æ®µ
    if epoch % 5 == 0:
        predictions = []
        references = []
        
        with torch.no_grad():
            for batch in val_dataloader:
                # ç”Ÿæˆç¿»è¯‘
                preds = model.generate(batch['src'])
                predictions.extend(preds)
                references.extend(batch['refs'])
        
        bleu_score = tracker.compute_bleu(
            predictions, references, 
            step=epoch, prefix='val'
        )
        print(f"Epoch {epoch}: Validation BLEU = {bleu_score:.2f}")
```

### 4.3 ä¸å…¶ä»–æŒ‡æ ‡è”åˆä½¿ç”¨

```python
import sacrebleu
from rouge import Rouge
import numpy as np

class ComprehensiveEvaluator:
    """ç»¼åˆè¯„ä¼°å™¨ï¼šBLEU + ROUGE"""
    
    def __init__(self):
        self.rouge = Rouge()
    
    def evaluate(
        self,
        predictions: List[str],
        references: List[List[str]]
    ) -> dict:
        """ç»¼åˆè¯„ä¼°"""
        # BLEU
        bleu = sacrebleu.corpus_bleu(predictions, references)
        
        # ROUGEï¼ˆå–ç¬¬ä¸€ä¸ªå‚è€ƒè¯‘æ–‡ï¼‰
        refs_for_rouge = [refs[0] for refs in references]
        rouge_scores = self.rouge.get_scores(
            predictions, refs_for_rouge, avg=True
        )
        
        return {
            'bleu': bleu.score,
            'bleu_details': {
                'bleu_1': bleu.precisions[0],
                'bleu_2': bleu.precisions[1],
                'bleu_3': bleu.precisions[2],
                'bleu_4': bleu.precisions[3],
                'bp': bleu.bp
            },
            'rouge_1': rouge_scores['rouge-1']['f'] * 100,
            'rouge_2': rouge_scores['rouge-2']['f'] * 100,
            'rouge_l': rouge_scores['rouge-l']['f'] * 100,
            'combined_score': (
                bleu.score * 0.5 + 
                rouge_scores['rouge-l']['f'] * 100 * 0.5
            )
        }
```

---

## ç¬¬5ç«  å‚è€ƒèµ„æ–™

### 5.1 å®˜æ–¹èµ„æº

- **SacreBLEUæ–‡æ¡£**: https://github.com/mjpost/sacrebleu
- **åŸå§‹è®ºæ–‡**: [BLEU: a Method for Automatic Evaluation of Machine Translation](https://aclanthology.org/P02-1040/)
- **NLTK BLEU**: https://www.nltk.org/api/nltk.translate.bleu_score.html

### 5.2 å®ç”¨æŠ€å·§

**æŠ€å·§1ï¼šæ ‡å‡†åŒ–é¢„å¤„ç†**

```python
import re

def normalize_text(text: str) -> str:
    """æ–‡æœ¬æ ‡å‡†åŒ–ï¼ˆä¸SacreBLEUä¿æŒä¸€è‡´ï¼‰"""
    # è½¬å°å†™
    text = text.lower()
    # å»é™¤å¤šä½™ç©ºæ ¼
    text = ' '.join(text.split())
    # æ ‡ç‚¹ç¬¦å·è§„èŒƒåŒ–
    text = re.sub(r'([.,!?])', r' \1 ', text)
    return text.strip()
```

**æŠ€å·§2ï¼šæ‰¹é‡è¯„ä¼°ä¼˜åŒ–**

```python
from multiprocessing import Pool
import sacrebleu

def compute_bleu_parallel(
    predictions: List[str],
    references: List[List[str]],
    num_workers: int = 4
) -> float:
    """å¹¶è¡Œè®¡ç®—BLEUï¼ˆå¤§æ•°æ®é›†ï¼‰"""
    # SacreBLEUæœ¬èº«å·²ä¼˜åŒ–ï¼Œé€šå¸¸ä¸éœ€è¦å¹¶è¡Œ
    # ä½†é¢„å¤„ç†å¯ä»¥å¹¶è¡Œ
    with Pool(num_workers) as pool:
        predictions = pool.map(normalize_text, predictions)
        references = [
            pool.map(normalize_text, refs) 
            for refs in references
        ]
    
    return sacrebleu.corpus_bleu(predictions, references).score
```

**æŠ€å·§3ï¼šç»“æœå¯è§†åŒ–**

```python
import matplotlib.pyplot as plt

def plot_bleu_history(history: List[dict], save_path: str = None):
    """ç»˜åˆ¶BLEUè®­ç»ƒæ›²çº¿"""
    steps = [h['step'] for h in history]
    bleu_scores = [h['bleu'] for h in history]
    bleu_1_scores = [h['bleu_1'] for h in history]
    bleu_4_scores = [h['bleu_4'] for h in history]
    
    plt.figure(figsize=(12, 6))
    plt.plot(steps, bleu_scores, label='BLEU', linewidth=2)
    plt.plot(steps, bleu_1_scores, label='BLEU-1', alpha=0.7)
    plt.plot(steps, bleu_4_scores, label='BLEU-4', alpha=0.7)
    plt.xlabel('Training Step')
    plt.ylabel('BLEU Score')
    plt.title('BLEU Score During Training')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.show()
```

### 5.4 å‘½ä»¤é€ŸæŸ¥è¡¨

**Python APIï¼š**

```python
import sacrebleu

# è¯­æ–™çº§BLEU
bleu = sacrebleu.corpus_bleu(hypotheses, references)

# å•å¥BLEU
bleu = sacrebleu.sentence_bleu(hypothesis, references)

# è‡ªå®šä¹‰å‚æ•°
bleu = sacrebleu.corpus_bleu(
    hypotheses, references,
    smooth_method='exp',      # å¹³æ»‘æ–¹æ³•
    max_ngram_order=4,        # æœ€å¤§n-gram
    tokenize='13a'            # åˆ†è¯æ–¹å¼
)
```

**å¸¸ç”¨åˆ†è¯æ–¹å¼ï¼š**

| å‚æ•° | è¯´æ˜ | é€‚ç”¨è¯­è¨€ |
|------|------|---------|
| `13a` | æ ‡å‡†åˆ†è¯ | è‹±è¯­ç­‰è¥¿æ–¹è¯­è¨€ |
| `zh` | ä¸­æ–‡åˆ†è¯ | ä¸­æ–‡ |
| `ja-mecab` | MeCabåˆ†è¯ | æ—¥è¯­ |
| `ko-mecab` | MeCabåˆ†è¯ | éŸ©è¯­ |
| `none` | ä¸åˆ†è¯ | å·²é¢„å¤„ç†æ•°æ® |

**BLEUåˆ†æ•°è§£è¯»ï¼š**

| åˆ†æ•°èŒƒå›´ | è´¨é‡è¯„ä¼° |
|---------|---------|
| 0-10 | å¾ˆå·®ï¼Œéš¾ä»¥ç†è§£ |
| 10-20 | å·®ï¼Œæœ‰å¤§é‡é”™è¯¯ |
| 20-30 | ä¸€èˆ¬ï¼ŒåŸºæœ¬å¯æ‡‚ |
| 30-40 | å¥½ï¼Œæµç•…åº¦è¾ƒå¥½ |
| 40-50 | å¾ˆå¥½ï¼Œæ¥è¿‘äººå·¥ |
| 50+ | ä¼˜ç§€ï¼Œéš¾ä»¥åŒºåˆ† |

---

## é™„å½•ï¼šå®Œæ•´é¡¹ç›®ç¤ºä¾‹

### ç¤ºä¾‹ï¼šä¸­è‹±ç¿»è¯‘æ¨¡å‹è¯„ä¼°

```python
import sacrebleu
import json
from datetime import datetime
from typing import List, Dict

class TranslationEvaluator:
    """å®Œæ•´çš„ç¿»è¯‘è¯„ä¼°ç³»ç»Ÿ"""
    
    def __init__(self, config: Dict = None):
        self.config = config or {}
        self.results_history = []
    
    def load_data(self, pred_file: str, ref_file: str) -> tuple:
        """åŠ è½½é¢„æµ‹å’Œå‚è€ƒæ–‡ä»¶"""
        with open(pred_file, 'r', encoding='utf-8') as f:
            predictions = [line.strip() for line in f]
        
        with open(ref_file, 'r', encoding='utf-8') as f:
            # æ”¯æŒå¤šå‚è€ƒï¼Œç”¨\tåˆ†éš”
            references = [
                line.strip().split('\t') 
                for line in f
            ]
        
        return predictions, references
    
    def evaluate(
        self,
        predictions: List[str],
        references: List[List[str]],
        model_name: str = "unknown"
    ) -> Dict:
        """æ‰§è¡Œå®Œæ•´è¯„ä¼°"""
        
        # è®¡ç®—BLEU
        bleu = sacrebleu.corpus_bleu(predictions, references)
        
        # è®¡ç®—æ¯ä¸ªå¥å­çš„BLEU
        sentence_bleus = [
            sacrebleu.sentence_bleu(pred, refs).score
            for pred, refs in zip(predictions, references)
        ]
        
        # ç»Ÿè®¡ä¿¡æ¯
        result = {
            'model_name': model_name,
            'timestamp': datetime.now().isoformat(),
            'num_sentences': len(predictions),
            'bleu': {
                'overall': bleu.score,
                'bleu_1': bleu.precisions[0],
                'bleu_2': bleu.precisions[1],
                'bleu_3': bleu.precisions[2],
                'bleu_4': bleu.precisions[3],
                'bp': bleu.bp,
                'sys_len': bleu.sys_len,
                'ref_len': bleu.ref_len
            },
            'sentence_level': {
                'mean': sum(sentence_bleus) / len(sentence_bleus),
                'median': sorted(sentence_bleus)[len(sentence_bleus)//2],
                'min': min(sentence_bleus),
                'max': max(sentence_bleus),
                'std': (sum((x - sum(sentence_bleus)/len(sentence_bleus))**2 
                           for x in sentence_bleus) / len(sentence_bleus))**0.5
            }
        }
        
        self.results_history.append(result)
        return result
    
    def print_report(self, result: Dict):
        """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
        print("=" * 60)
        print(f"ç¿»è¯‘è´¨é‡è¯„ä¼°æŠ¥å‘Š - {result['model_name']}")
        print(f"è¯„ä¼°æ—¶é—´: {result['timestamp']}")
        print("=" * 60)
        print(f"è¯„ä¼°å¥æ•°: {result['num_sentences']}")
        print()
        print("ã€æ•´ä½“BLEUåˆ†æ•°ã€‘")
        print(f"  BLEU:  {result['bleu']['overall']:.2f}")
        print(f"  BLEU-1: {result['bleu']['bleu_1']:.2f}")
        print(f"  BLEU-2: {result['bleu']['bleu_2']:.2f}")
        print(f"  BLEU-3: {result['bleu']['bleu_3']:.2f}")
        print(f"  BLEU-4: {result['bleu']['bleu_4']:.2f}")
        print(f"  BP: {result['bleu']['bp']:.4f}")
        print()
        print("ã€å¥å­çº§ç»Ÿè®¡ã€‘")
        print(f"  å¹³å‡BLEU: {result['sentence_level']['mean']:.2f}")
        print(f"  ä¸­ä½æ•°: {result['sentence_level']['median']:.2f}")
        print(f"  æœ€å°å€¼: {result['sentence_level']['min']:.2f}")
        print(f"  æœ€å¤§å€¼: {result['sentence_level']['max']:.2f}")
        print(f"  æ ‡å‡†å·®: {result['sentence_level']['std']:.2f}")
        print("=" * 60)
    
    def save_report(self, result: Dict, output_file: str):
        """ä¿å­˜è¯„ä¼°æŠ¥å‘Šåˆ°JSON"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"æŠ¥å‘Šå·²ä¿å­˜: {output_file}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    evaluator = TranslationEvaluator()
    
    # ç¤ºä¾‹æ•°æ®
    predictions = [
        "hello world",
        "machine translation is useful",
        "deep learning improves quality"
    ]
    
    references = [
        ["hello world", "hi world"],
        ["machine translation is helpful", "machine translation is useful"],
        ["deep learning improves the quality"]
    ]
    
    # æ‰§è¡Œè¯„ä¼°
    result = evaluator.evaluate(
        predictions, references,
        model_name="Transformer_Base"
    )
    
    # æ‰“å°æŠ¥å‘Š
    evaluator.print_report(result)
    
    # ä¿å­˜æŠ¥å‘Š
    evaluator.save_report(result, 'translation_eval_report.json')
```

**è¿è¡Œæ­¥éª¤ï¼š**

```bash
# 1. å‡†å¤‡æ•°æ®æ–‡ä»¶
# predictions.txt: æ¯è¡Œä¸€ä¸ªé¢„æµ‹è¯‘æ–‡
# references.txt: æ¯è¡Œä¸€ä¸ªå‚è€ƒè¯‘æ–‡ï¼ˆå¤šå‚è€ƒç”¨tabåˆ†éš”ï¼‰

# 2. è¿è¡Œè¯„ä¼°
python bleu_evaluation.py

# 3. æŸ¥çœ‹æŠ¥å‘Š
# ç»ˆç«¯è¾“å‡º + translation_eval_report.json
```

**é¢„æœŸç»“æœï¼š**
- ğŸ“Š è¯¦ç»†çš„BLEUåˆ†æ•°ï¼ˆæ•´ä½“+å„n-gramï¼‰
- ğŸ“ˆ å¥å­çº§ç»Ÿè®¡ä¿¡æ¯
- ğŸ“ JSONæ ¼å¼çš„å®Œæ•´æŠ¥å‘Š
- ğŸ¯ ä¾¿äºå¯¹æ¯”ä¸åŒæ¨¡å‹çš„ç»“æœ

---

**æ­å–œï¼** ä½ å·²ç»æŒæ¡äº†BLEUè¯„ä¼°æŒ‡æ ‡çš„åŸºç¡€ä½¿ç”¨ã€‚ç»§ç»­æ¢ç´¢æ›´å¤šé«˜çº§åŠŸèƒ½ï¼Œè®©ç¿»è¯‘è´¨é‡è¯„ä¼°æ›´åŠ ç§‘å­¦å’Œå…¨é¢ï¼

**ä¸‹ä¸€æ­¥å»ºè®®ï¼š**
1. å°è¯•åœ¨è‡ªå·±çš„ç¿»è¯‘æ¨¡å‹ä¸Šä½¿ç”¨BLEUè¯„ä¼°
2. æ¢ç´¢å…¶ä»–è¯„ä¼°æŒ‡æ ‡ï¼ˆROUGEã€METEORã€BERTScoreï¼‰
3. å­¦ä¹ å¦‚ä½•è¿›è¡Œäººå·¥è¯„ä¼°ä¸è‡ªåŠ¨è¯„ä¼°çš„ç›¸å…³æ€§åˆ†æ
4. ç ”ç©¶é¢†åŸŸç‰¹å®šçš„è¯„ä¼°æ–¹æ³•ï¼ˆåŒ»å­¦ã€æ³•å¾‹ç¿»è¯‘ï¼‰

**Happy Evaluating! ğŸ“Š**
