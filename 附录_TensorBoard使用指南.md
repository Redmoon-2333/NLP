# é™„å½•ï¼šTensorBoardä½¿ç”¨æŒ‡å—

## ç¬¬1ç«  æ¦‚è¿°

### 1.1 ä»€ä¹ˆæ˜¯TensorBoardï¼Ÿ

**TensorBoard**æ˜¯TensorFlowæä¾›çš„ä¸€å¥—å¯è§†åŒ–å·¥å…·ï¼Œç”¨äºç†è§£ã€è°ƒè¯•å’Œä¼˜åŒ–æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚å®ƒèƒ½å¤Ÿå°†è®­ç»ƒè¿‡ç¨‹ä¸­çš„å„ç§æŒ‡æ ‡ä»¥ç›´è§‚çš„å›¾è¡¨å½¢å¼å±•ç¤ºå‡ºæ¥ï¼Œå¸®åŠ©å¼€å‘è€…ç›‘æ§æ¨¡å‹è®­ç»ƒçŠ¶æ€ã€åˆ†ææ¨¡å‹æ€§èƒ½ã€è°ƒè¯•é—®é¢˜ã€‚

**æ ¸å¿ƒåŠŸèƒ½ï¼š**
- ğŸ“Š **å¯è§†åŒ–è®­ç»ƒæŒ‡æ ‡**ï¼šå®æ—¶æŸ¥çœ‹lossã€accuracyç­‰æŒ‡æ ‡çš„å˜åŒ–è¶‹åŠ¿
- ğŸ” **æ¨¡å‹ç»“æ„å¯è§†åŒ–**ï¼šæŸ¥çœ‹ç½‘ç»œç»“æ„å’Œæ•°æ®æµå‘
- ğŸ“ˆ **å‚æ•°åˆ†å¸ƒå¯è§†åŒ–**ï¼šç›‘æ§æƒé‡ã€æ¢¯åº¦çš„ç»Ÿè®¡åˆ†å¸ƒ
- ğŸ–¼ï¸ **å›¾åƒå’Œæ–‡æœ¬å¯è§†åŒ–**ï¼šæŸ¥çœ‹æ¨¡å‹å¤„ç†çš„æ•°æ®å’Œç”Ÿæˆç»“æœ
- ğŸ¯ **è¶…å‚æ•°è°ƒä¼˜**ï¼šå¯¹æ¯”ä¸åŒè¶…å‚æ•°é…ç½®ä¸‹çš„è®­ç»ƒæ•ˆæœ

**é€‚ç”¨æ¡†æ¶ï¼š**
è™½ç„¶TensorBoardç”±TensorFlowå¼€å‘ï¼Œä½†å®ƒå·²ç»æˆä¸ºæ·±åº¦å­¦ä¹ é¢†åŸŸçš„é€šç”¨å·¥å…·ï¼Œæ”¯æŒï¼š
- PyTorchï¼ˆé€šè¿‡torch.utils.tensorboardï¼‰
- Keras
- MXNet
- å…¶ä»–æ¡†æ¶

### 1.2 ä¸ºä»€ä¹ˆéœ€è¦TensorBoardï¼Ÿ

**è®­ç»ƒè¿‡ç¨‹ç›‘æ§çš„æŒ‘æˆ˜ï¼š**

åœ¨è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬é¢ä¸´ä»¥ä¸‹é—®é¢˜ï¼š

| é—®é¢˜ | è¯´æ˜ | TensorBoardçš„è§£å†³æ–¹æ¡ˆ |
|------|------|---------------------|
| **æ— æ³•å®æ—¶è§‚å¯Ÿè®­ç»ƒçŠ¶æ€** | åªèƒ½çœ‹åˆ°ç»ˆç«¯è¾“å‡ºçš„æ•°å­— | å®æ—¶å›¾è¡¨å±•ç¤ºè®­ç»ƒæ›²çº¿ |
| **éš¾ä»¥å¯¹æ¯”å®éªŒ** | ä¸åŒå®éªŒçš„ç»“æœéš¾ä»¥æ¯”è¾ƒ | åœ¨åŒä¸€å›¾è¡¨ä¸­å åŠ å¤šæ¡æ›²çº¿ |
| **è°ƒè¯•å›°éš¾** | ä¸çŸ¥é“æ¨¡å‹å“ªé‡Œå‡ºäº†é—®é¢˜ | å¯è§†åŒ–æ¢¯åº¦ã€æƒé‡åˆ†å¸ƒ |
| **ç»“æœä¸ç›´è§‚** | æ•°å­—å †ç Œï¼Œç¼ºä¹è§†è§‰å†²å‡» | å›¾è¡¨ã€å›¾åƒã€æ–‡æœ¬å±•ç¤º |

**ä½¿ç”¨å‰åå¯¹æ¯”ï¼š**

```
âŒ ä¸ä½¿ç”¨TensorBoardï¼š
Epoch 1/10, Loss: 2.3456, Acc: 0.5234
Epoch 2/10, Loss: 1.8923, Acc: 0.6123
Epoch 3/10, Loss: 1.4567, Acc: 0.6891
...

âœ… ä½¿ç”¨TensorBoardï¼š
ğŸ“Š å¹³æ»‘çš„Lossä¸‹é™æ›²çº¿
ğŸ“ˆ æ¸…æ™°çš„Accuracyä¸Šå‡è¶‹åŠ¿
ğŸ” å®æ—¶ç›‘æ§ï¼Œå‘ç°å¼‚å¸¸ç«‹å³å¤„ç†
ğŸ“‹ å¤šä¸ªå®éªŒå¯¹æ¯”ä¸€ç›®äº†ç„¶
```

**å…¸å‹åº”ç”¨åœºæ™¯ï¼š**
1. **è®­ç»ƒç›‘æ§**ï¼šå®æ—¶æŸ¥çœ‹lossæ˜¯å¦æ”¶æ•›
2. **è¶…å‚æ•°è°ƒä¼˜**ï¼šå¯¹æ¯”ä¸åŒå­¦ä¹ ç‡ã€batch sizeçš„æ•ˆæœ
3. **æ¨¡å‹è¯Šæ–­**ï¼šæ£€æŸ¥æ˜¯å¦è¿‡æ‹Ÿåˆã€æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
4. **ç»“æœå±•ç¤º**ï¼šå‘å›¢é˜Ÿæˆ–è®ºæ–‡å±•ç¤ºè®­ç»ƒæ•ˆæœ

---

## ç¬¬2ç«  å®‰è£…TensorBoard

### 2.1 å®‰è£…æ–¹æ³•

**æ–¹æ³•ä¸€ï¼šé€šè¿‡pipå®‰è£…ï¼ˆæ¨èï¼‰**

```bash
# å®‰è£…TensorBoard
pip install tensorboard

# éªŒè¯å®‰è£…
tensorboard --version
```

**æ–¹æ³•äºŒï¼šéšPyTorchä¸€èµ·å®‰è£…**

å¦‚æœä½¿ç”¨PyTorchï¼ŒTensorBoardæ”¯æŒé€šå¸¸å·²åŒ…å«ï¼š

```bash
# PyTorchç¯å¢ƒä¸­
pip install torch torchvision
# tensorboardå·²åŒ…å«åœ¨ä¾èµ–ä¸­

# æˆ–æ˜ç¡®å®‰è£…
pip install tensorboard
```

**æ–¹æ³•ä¸‰ï¼šåœ¨è™šæ‹Ÿç¯å¢ƒä¸­å®‰è£…**

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n myenv python=3.8
conda activate myenv

# å®‰è£…TensorBoard
pip install tensorboard
```

### 2.2 éªŒè¯å®‰è£…

**æµ‹è¯•TensorBoardæ˜¯å¦æ­£ç¡®å®‰è£…ï¼š**

```python
# åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•è„šæœ¬ test_tensorboard.py
from torch.utils.tensorboard import SummaryWriter
import numpy as np

# åˆ›å»ºå†™å…¥å™¨
writer = SummaryWriter('runs/test')

# å†™å…¥ä¸€äº›æµ‹è¯•æ•°æ®
for i in range(100):
    writer.add_scalar('test/loss', np.sin(i/10), i)

writer.close()
print("æµ‹è¯•æ•°æ®å·²å†™å…¥ï¼Œè¯·è¿è¡Œ: tensorboard --logdir=runs")
```

**è¿è¡Œæµ‹è¯•ï¼š**

```bash
# 1. è¿è¡Œæµ‹è¯•è„šæœ¬
python test_tensorboard.py

# 2. å¯åŠ¨TensorBoard
tensorboard --logdir=runs

# 3. åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€
# é€šå¸¸æ˜¯ http://localhost:6006
```

å¦‚æœèƒ½çœ‹åˆ°ä¸€æ¡æ­£å¼¦æ›²çº¿ï¼Œè¯´æ˜å®‰è£…æˆåŠŸï¼

### 2.3 å¸¸è§å®‰è£…é—®é¢˜

**é—®é¢˜1ï¼šç«¯å£è¢«å ç”¨**

```bash
# é”™è¯¯ä¿¡æ¯
TensorBoard attempted to bind to port 6006, but it was already in use

# è§£å†³æ–¹æ¡ˆï¼šæŒ‡å®šå…¶ä»–ç«¯å£
tensorboard --logdir=runs --port=6007
```

**é—®é¢˜2ï¼šæ‰¾ä¸åˆ°tensorboardå‘½ä»¤**

```bash
# ç¡®è®¤å®‰è£…è·¯å¾„
pip show tensorboard

# å°†tensorboardæ·»åŠ åˆ°PATH
# Windows:
set PATH=%PATH%;C:\Users\YourName\AppData\Local\Programs\Python\Python38\Scripts

# Linux/Mac:
export PATH=$PATH:~/.local/bin
```

**é—®é¢˜3ï¼šç‰ˆæœ¬å†²çª**

```bash
# å¸è½½æ—§ç‰ˆæœ¬
pip uninstall tensorboard

# é‡æ–°å®‰è£…
pip install tensorboard
```

---

## ç¬¬3ç«  åŸºç¡€ä½¿ç”¨

### 3.1 æ¦‚è¿°

ä½¿ç”¨TensorBoardçš„åŸºæœ¬æµç¨‹åŒ…æ‹¬ä¸‰ä¸ªæ­¥éª¤ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. è®°å½•æ•°æ®     â”‚  â†’   â”‚  2. å¯åŠ¨æœåŠ¡     â”‚  â†’   â”‚  3. æµè§ˆå™¨æŸ¥çœ‹   â”‚
â”‚  (è®­ç»ƒä»£ç ä¸­)    â”‚      â”‚  (å‘½ä»¤è¡Œ)        â”‚      â”‚  (Webç•Œé¢)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  SummaryWriter         tensorboard --logdir      http://localhost:6006
```

### 3.2 åˆ›å»ºSummaryWriter

**SummaryWriter**æ˜¯TensorBoardçš„æ ¸å¿ƒç±»ï¼Œè´Ÿè´£å°†è®­ç»ƒæ•°æ®å†™å…¥æ—¥å¿—æ–‡ä»¶ã€‚

**åŸºæœ¬ç”¨æ³•ï¼š**

```python
from torch.utils.tensorboard import SummaryWriter

# åˆ›å»ºå†™å…¥å™¨ï¼ˆæŒ‡å®šæ—¥å¿—ç›®å½•ï¼‰
writer = SummaryWriter('runs/experiment_1')

# ... è®­ç»ƒä»£ç  ...

# å…³é—­å†™å…¥å™¨
writer.close()
```

**å‚æ•°è¯´æ˜ï¼š**

```python
writer = SummaryWriter(
    log_dir='runs/my_experiment',  # æ—¥å¿—ä¿å­˜ç›®å½•
    comment='learning_rate_0.001', # å®éªŒå¤‡æ³¨ï¼ˆä¼šæ·»åŠ åˆ°ç›®å½•åï¼‰
    flush_secs=10                  # å¤šå°‘ç§’åˆ·æ–°ä¸€æ¬¡åˆ°ç£ç›˜
)
```

**ç›®å½•ç»“æ„ç¤ºä¾‹ï¼š**

```
é¡¹ç›®æ ¹ç›®å½•/
â””â”€â”€ runs/                          # é»˜è®¤æ—¥å¿—æ ¹ç›®å½•
    â”œâ”€â”€ experiment_1/              # å®éªŒ1çš„æ—¥å¿—
    â”‚   â””â”€â”€ events.out.tfevents.*  # TensorBoardäº‹ä»¶æ–‡ä»¶
    â”œâ”€â”€ experiment_2/              # å®éªŒ2çš„æ—¥å¿—
    â””â”€â”€ Jan01_12-00-00_hostname/   # è‡ªåŠ¨å‘½åçš„å®éªŒ
```

**æœ€ä½³å®è·µï¼š**

```python
import os
from datetime import datetime
from torch.utils.tensorboard import SummaryWriter

# ä½¿ç”¨æ—¶é—´æˆ³å‘½åï¼Œé¿å…è¦†ç›–
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
log_dir = os.path.join('runs', f'exp_{timestamp}')
writer = SummaryWriter(log_dir)

print(f"TensorBoardæ—¥å¿—ä¿å­˜åœ¨: {log_dir}")
print(f"å¯åŠ¨å‘½ä»¤: tensorboard --logdir=runs")
```

### 3.3 è®°å½•æ ‡é‡ï¼ˆScalarï¼‰

**æ ‡é‡**æ˜¯æœ€å¸¸ç”¨çš„è®°å½•ç±»å‹ï¼Œç”¨äºè®°å½•å•ä¸ªæ•°å€¼ï¼ˆå¦‚lossã€accuracyï¼‰éšæ—¶é—´çš„å˜åŒ–ã€‚

**åŸºæœ¬ç”¨æ³•ï¼š**

```python
# add_scalar(tag, scalar_value, global_step)
# tag: æ ‡ç­¾åç§°
# scalar_value: è¦è®°å½•çš„æ•°å€¼
# global_step: æ¨ªè½´åæ ‡ï¼ˆé€šå¸¸æ˜¯è¿­ä»£æ¬¡æ•°æˆ–epochï¼‰

writer.add_scalar('Loss/train', loss.item(), epoch)
writer.add_scalar('Accuracy/train', acc, epoch)
```

**å®Œæ•´è®­ç»ƒç¤ºä¾‹ï¼š**

```python
import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter

# åˆ›å»ºå†™å…¥å™¨
writer = SummaryWriter('runs/mnist_experiment')

# æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
model = nn.Linear(10, 1)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
criterion = nn.MSELoss()

for epoch in range(100):
    # æ¨¡æ‹Ÿå‰å‘ä¼ æ’­
    inputs = torch.randn(32, 10)
    labels = torch.randn(32, 1)
    
    # è®¡ç®—æŸå¤±
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # è®°å½•åˆ°TensorBoard
    writer.add_scalar('Loss/train', loss.item(), epoch)
    
    if epoch % 10 == 0:
        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')

writer.close()
```

**ç»„ç»‡æ ‡é‡çš„å‘½åè§„èŒƒï¼š**

ä½¿ç”¨`/`åˆ†éš”å¯ä»¥åˆ›å»ºå±‚æ¬¡ç»“æ„ï¼ŒTensorBoardä¼šè‡ªåŠ¨åˆ†ç»„ï¼š

```python
# è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†å¼€
writer.add_scalar('Loss/train', train_loss, step)
writer.add_scalar('Loss/test', test_loss, step)

# ä¸åŒæŒ‡æ ‡åˆ†ç»„
writer.add_scalar('Metrics/accuracy', acc, step)
writer.add_scalar('Metrics/precision', prec, step)
writer.add_scalar('Metrics/recall', recall, step)

# å¤šä»»åŠ¡å­¦ä¹ 
writer.add_scalar('Task1/loss', task1_loss, step)
writer.add_scalar('Task2/loss', task2_loss, step)
```

**åœ¨TensorBoardä¸­çš„å±•ç¤ºï¼š**

```
ğŸ“Š SCALARSæ ‡ç­¾é¡µ
â”œâ”€ Loss
â”‚  â”œâ”€ train  ğŸ“ˆ (è®­ç»ƒlossæ›²çº¿)
â”‚  â””â”€ test   ğŸ“ˆ (æµ‹è¯•lossæ›²çº¿)
â””â”€ Metrics
   â”œâ”€ accuracy  ğŸ“ˆ
   â”œâ”€ precision ğŸ“ˆ
   â””â”€ recall    ğŸ“ˆ
```

### 3.4 è‡ªåŠ¨TensorBoardæœåŠ¡

**æ‰‹åŠ¨å¯åŠ¨TensorBoardï¼š**

```bash
# åŸºæœ¬å‘½ä»¤
tensorboard --logdir=runs

# æŒ‡å®šç«¯å£
tensorboard --logdir=runs --port=6007

# æŒ‡å®šä¸»æœºï¼ˆå…è®¸è¿œç¨‹è®¿é—®ï¼‰
tensorboard --logdir=runs --host=0.0.0.0

# åå°è¿è¡Œ
nohup tensorboard --logdir=runs &
```

**åœ¨Jupyter Notebookä¸­ä½¿ç”¨ï¼š**

```python
# æ–¹æ³•1ï¼šä½¿ç”¨é­”æ³•å‘½ä»¤ï¼ˆæ¨èï¼‰
%load_ext tensorboard
%tensorboard --logdir runs

# æ–¹æ³•2ï¼šä½¿ç”¨notebookæ¨¡å—
from tensorboard import notebook
notebook.start("--logdir runs")
```

**åœ¨Pythonä»£ç ä¸­è‡ªåŠ¨å¯åŠ¨ï¼š**

```python
import subprocess
import webbrowser
import time

def start_tensorboard(logdir='runs', port=6006):
    """è‡ªåŠ¨å¯åŠ¨TensorBoardå¹¶æ‰“å¼€æµè§ˆå™¨"""
    # å¯åŠ¨TensorBoardè¿›ç¨‹
    process = subprocess.Popen(
        ['tensorboard', '--logdir', logdir, '--port', str(port)],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    )
    
    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    time.sleep(3)
    
    # è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
    url = f'http://localhost:{port}'
    webbrowser.open(url)
    
    print(f"TensorBoardå·²å¯åŠ¨: {url}")
    return process

# ä½¿ç”¨
tb_process = start_tensorboard()

# è®­ç»ƒç»“æŸåå…³é—­
# tb_process.terminate()
```

---

## ç¬¬4ç«  å‚è€ƒèµ„æ–™

### 4.1 å®˜æ–¹æ–‡æ¡£

- **TensorBoardå®˜æ–¹æ–‡æ¡£**: https://www.tensorflow.org/tensorboard
- **PyTorch TensorBoardæ•™ç¨‹**: https://pytorch.org/docs/stable/tensorboard.html
- **GitHubä»“åº“**: https://github.com/tensorflow/tensorboard

### 4.2 å¸¸ç”¨èµ„æº

**æ•™ç¨‹å’Œç¤ºä¾‹ï¼š**
- TensorFlowå®˜æ–¹æ•™ç¨‹ï¼šå®Œæ•´çš„ä½¿ç”¨æŒ‡å—å’Œæœ€ä½³å®è·µ
- PyTorchå®˜æ–¹ç¤ºä¾‹ï¼šPyTorché›†æˆTensorBoardçš„ç¤ºä¾‹ä»£ç 
- Kerasæ–‡æ¡£ï¼šKerasä¸­ä½¿ç”¨TensorBoardå›è°ƒ

**ç¤¾åŒºèµ„æºï¼š**
- Stack Overflowï¼šå¸¸è§é—®é¢˜è§£ç­”
- Reddit r/MachineLearningï¼šç»éªŒåˆ†äº«
- çŸ¥ä¹ä¸“æ ï¼šä¸­æ–‡æ•™ç¨‹å’Œæ¡ˆä¾‹

### 4.3 å®ç”¨æŠ€å·§

**æŠ€å·§1ï¼šå®éªŒç®¡ç†**

```python
# ä½¿ç”¨é…ç½®å­—å…¸ç»„ç»‡å®éªŒ
config = {
    'lr': 0.001,
    'batch_size': 64,
    'optimizer': 'Adam'
}

# å°†é…ç½®ç¼–ç åˆ°ç›®å½•å
from urllib.parse import urlencode
config_str = urlencode(config)
log_dir = f'runs/exp_{config_str}'
writer = SummaryWriter(log_dir)
```

**æŠ€å·§2ï¼šæ¸…ç†æ—§æ—¥å¿—**

```python
import shutil

# åˆ é™¤æ—§çš„å®éªŒæ—¥å¿—
def clean_old_runs(keep_recent=5):
    runs_dir = 'runs'
    subdirs = sorted(os.listdir(runs_dir))
    
    if len(subdirs) > keep_recent:
        for old_dir in subdirs[:-keep_recent]:
            shutil.rmtree(os.path.join(runs_dir, old_dir))
            print(f"å·²åˆ é™¤æ—§æ—¥å¿—: {old_dir}")
```

**æŠ€å·§3ï¼šå¤šGPUè®­ç»ƒä¸­åªåœ¨ä¸»è¿›ç¨‹è®°å½•**

```python
import torch.distributed as dist

# åªåœ¨rank 0è¿›ç¨‹è®°å½•
if not dist.is_initialized() or dist.get_rank() == 0:
    writer = SummaryWriter('runs/experiment')
else:
    writer = None

# è®­ç»ƒä¸­
if writer is not None:
    writer.add_scalar('Loss/train', loss.item(), step)
```

**æŠ€å·§4ï¼šä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨**

```python
from contextlib import contextmanager

@contextmanager
def create_summary_writer(log_dir):
    writer = SummaryWriter(log_dir)
    try:
        yield writer
    finally:
        writer.close()
        print(f"TensorBoardæ—¥å¿—å·²ä¿å­˜åˆ°: {log_dir}")

# ä½¿ç”¨
with create_summary_writer('runs/exp') as writer:
    for epoch in range(100):
        # è®­ç»ƒä»£ç 
        writer.add_scalar('Loss', loss, epoch)
```

### 4.4 å‘½ä»¤é€ŸæŸ¥è¡¨

**å¯åŠ¨å‘½ä»¤ï¼š**

```bash
# åŸºæœ¬å¯åŠ¨
tensorboard --logdir=runs

# æŒ‡å®šç«¯å£
tensorboard --logdir=runs --port=6007

# å…è®¸è¿œç¨‹è®¿é—®
tensorboard --logdir=runs --host=0.0.0.0

# åå°è¿è¡Œ
tensorboard --logdir=runs &

# æŸ¥çœ‹ç‰ˆæœ¬
tensorboard --version

# æŸ¥çœ‹å¸®åŠ©
tensorboard --help
```

**å¸¸ç”¨Python APIï¼š**

```python
from torch.utils.tensorboard import SummaryWriter

# åˆ›å»ºå†™å…¥å™¨
writer = SummaryWriter('runs/exp')

# è®°å½•æ ‡é‡
writer.add_scalar('Loss', loss, step)

# è®°å½•å¤šä¸ªæ ‡é‡
writer.add_scalars('Losses', {'train': train_loss, 'test': test_loss}, step)

# è®°å½•ç›´æ–¹å›¾
writer.add_histogram('weights', model.fc.weight, step)

# è®°å½•å›¾åƒ
writer.add_image('Image', img_tensor, step)

# è®°å½•å›¾
writer.add_graph(model, input_tensor)

# è®°å½•æ–‡æœ¬
writer.add_text('Config', 'Learning rate: 0.001', step)

# å…³é—­å†™å…¥å™¨
writer.close()
```

---

## é™„å½•ï¼šå®Œæ•´é¡¹ç›®ç¤ºä¾‹

### ç¤ºä¾‹ï¼šä½¿ç”¨TensorBoardç›‘æ§RNNè®­ç»ƒ

```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime
import os

class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        out, _ = self.rnn(x)
        out = self.fc(out[:, -1, :])
        return out

def train_with_tensorboard():
    # 1. åˆ›å»ºTensorBoardå†™å…¥å™¨
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_dir = os.path.join('runs', f'rnn_experiment_{timestamp}')
    writer = SummaryWriter(log_dir)
    print(f"TensorBoardæ—¥å¿—ç›®å½•: {log_dir}")
    print(f"å¯åŠ¨å‘½ä»¤: tensorboard --logdir=runs")
    
    # 2. æ¨¡å‹å’Œè®­ç»ƒé…ç½®
    model = SimpleRNN(input_size=10, hidden_size=20, output_size=2)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # 3. è®°å½•è¶…å‚æ•°
    hparams = {
        'lr': 0.001,
        'batch_size': 32,
        'hidden_size': 20,
        'optimizer': 'Adam'
    }
    writer.add_text('Hyperparameters', str(hparams), 0)
    
    # 4. è®­ç»ƒå¾ªç¯
    for epoch in range(100):
        # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
        inputs = torch.randn(32, 5, 10)  # (batch, seq_len, input_size)
        labels = torch.randint(0, 2, (32,))  # (batch,)
        
        # å‰å‘ä¼ æ’­
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        
        # è®¡ç®—å‡†ç¡®ç‡
        _, predicted = torch.max(outputs, 1)
        accuracy = (predicted == labels).float().mean()
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # 5. è®°å½•åˆ°TensorBoard
        writer.add_scalar('Loss/train', loss.item(), epoch)
        writer.add_scalar('Accuracy/train', accuracy.item(), epoch)
        
        # è®°å½•å­¦ä¹ ç‡
        current_lr = optimizer.param_groups[0]['lr']
        writer.add_scalar('Learning_Rate', current_lr, epoch)
        
        # æ¯10ä¸ªepochè®°å½•æƒé‡åˆ†å¸ƒ
        if epoch % 10 == 0:
            for name, param in model.named_parameters():
                writer.add_histogram(f'Parameters/{name}', param, epoch)
                if param.grad is not None:
                    writer.add_histogram(f'Gradients/{name}', param.grad, epoch)
        
        if epoch % 10 == 0:
            print(f'Epoch {epoch}: Loss={loss.item():.4f}, Acc={accuracy.item():.4f}')
    
    # 6. å…³é—­å†™å…¥å™¨
    writer.close()
    print("è®­ç»ƒå®Œæˆï¼è¯·åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹TensorBoard")

if __name__ == '__main__':
    train_with_tensorboard()
```

**è¿è¡Œæ­¥éª¤ï¼š**

```bash
# 1. è¿è¡Œè®­ç»ƒè„šæœ¬
python train.py

# 2. å¯åŠ¨TensorBoardï¼ˆåœ¨å¦ä¸€ä¸ªç»ˆç«¯ï¼‰
tensorboard --logdir=runs

# 3. åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€
# http://localhost:6006
```

**é¢„æœŸç»“æœï¼š**
- ğŸ“‰ Lossæ›²çº¿å¹³æ»‘ä¸‹é™
- ğŸ“ˆ Accuracyæ›²çº¿ç¨³æ­¥ä¸Šå‡
- ğŸ“Š æƒé‡å’Œæ¢¯åº¦çš„åˆ†å¸ƒç›´æ–¹å›¾
- ğŸ“ è¶…å‚æ•°é…ç½®è®°å½•

---

**æ­å–œï¼** ä½ å·²ç»æŒæ¡äº†TensorBoardçš„åŸºç¡€ä½¿ç”¨ã€‚ç»§ç»­æ¢ç´¢æ›´å¤šé«˜çº§åŠŸèƒ½ï¼Œè®©æ¨¡å‹è®­ç»ƒè¿‡ç¨‹æ›´åŠ é€æ˜å’Œå¯æ§ï¼

**ä¸‹ä¸€æ­¥å»ºè®®ï¼š**
1. å°è¯•åœ¨è‡ªå·±çš„é¡¹ç›®ä¸­é›†æˆTensorBoard
2. æ¢ç´¢å›¾åƒã€éŸ³é¢‘ç­‰å¤šåª’ä½“æ•°æ®çš„å¯è§†åŒ–
3. å­¦ä¹ ä½¿ç”¨TensorBoardè¿›è¡Œè¶…å‚æ•°è°ƒä¼˜
4. ç ”ç©¶å¦‚ä½•åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨TensorBoardç›‘æ§æ¨¡å‹

**Happy Visualizing! ğŸ“Š**
